## 0908 (~page 40)

### 1. 대용량 데이터 ETL - 서버리스 솔루션

**문제 시나리오**: Amazon S3에 있는 수 테라바이트 원시 데이터를 정리/준비 후 Redshift로 로드, 복잡한 쿼리 수행, 최소 운영 오버헤드

**정답: AWS Glue DataBrew + AWS Glue + Amazon Redshift**

**왜 이 조합이 최적인가:**

**A. AWS Glue DataBrew의 역할:**
```
원시 데이터 → 시각적 데이터 정리 → 준비된 데이터
```

**DataBrew의 장점:**
- **코드 없는 데이터 정리**: 200+ 내장 변환 기능
- **시각적 프로파일링**: 데이터 품질 자동 분석
- **스키마 추론**: 자동 데이터 타입 감지
- **확장성**: 테라바이트급 데이터 처리

**B. AWS Glue ETL의 역할:**
```
준비된 데이터 → 스키마 변환 → Redshift 최적화 로드
```

**C. 전체 아키텍처:**
```
S3 Raw Data → DataBrew (정리/준비) → S3 Prepared Data → Glue ETL → Redshift → 분석가 쿼리
     ↑              ↑ 코드 없음              ↑                    ↑ 자동 스키마        ↑ 복잡 쿼리
수 TB 데이터      200+ 변환 기능         Parquet 최적화        최적화 로드        고성능 분석
```

**왜 다른 옵션들이 부적절한가:**

**직접 Glue ETL만 사용:**
- ❌ 데이터 정리를 위한 복잡한 코드 작성 필요
- ❌ 데이터 프로파일링 직접 구현
- ❌ 높은 개발/운영 오버헤드

**Spark on EMR:**
- ❌ 클러스터 관리 필요 (인프라 운영)
- ❌ 확장성 수동 관리
- ❌ 높은 운영 오버헤드

**Lambda 기반 처리:**
- ❌ 15분 실행 제한으로 대용량 처리 불가
- ❌ 메모리 제한 (최대 10GB)

**DataBrew + Glue 조합의 핵심 장점:**
1. **완전 서버리스**: 인프라 관리 불필요
2. **자동 확장**: 데이터 크기에 따라 자동 스케일링
3. **최소 개발**: 시각적 도구 + 관리형 서비스
4. **비용 최적화**: 사용한 만큼만 과금
5. **Redshift 통합**: 네이티브 최적화 로드

**비용 분석:**
```
DataBrew: $1 per node hour + $0.012 per GB processed
Glue ETL: $0.44 per DPU hour
Redshift: 기존 클러스터 활용
```

시험 포인트:
- **대용량 데이터 + 최소 운영 오버헤드** = DataBrew + Glue
- **코드 없는 데이터 정리** = DataBrew 활용
- **서버리스 ETL** = 완전 관리형 서비스 조합

### 2. AWS Glue FindMatches - 중복 레코드 연결

**문제 시나리오**: 여러 RDS 데이터베이스의 고객 레코드 연결, 필드명 불일치 (place_id vs location_id 등), 최소 운영 오버헤드

**정답: AWS Glue Crawler + FindMatches Transform**

**FindMatches Transform의 핵심 기능:**

**A. ML 기반 중복 감지:**
- FindMatches Transform 생성하여 고객 레코드 매칭
- 훈련 데이터로 ML 모델 학습
- Precision/Recall 메트릭으로 매칭 품질 평가

**왜 FindMatches가 최적인가:**

**자동 필드 매핑:**
- **스마트 매칭**: 필드명이 달라도 내용 기반 매칭
- **퍼지 매칭**: 오타, 약어, 형식 차이 처리
- **컨텍스트 이해**: 여러 필드 조합으로 판단

**예시 매칭 케이스:**
- DB1의 "place_id" ↔ DB2의 "location_id" 자동 매칭
- "John Smith" ↔ "J. Smith" 이름 변형 처리
- "555-1234" ↔ "(555) 123-4567" 전화번호 형식 차이 극복

**대안 방식들의 한계:**
- **직접 SQL JOIN**: 필드명이 다르면 매칭 실패
- **규칙 기반 매칭**: 모든 경우의 수를 코드로 작성해야 하므로 복잡하고 유지보수 어려움

시험 포인트:
- **필드명 불일치 + 레코드 연결** = FindMatches Transform
- **ML 기반 중복 감지** = 자동 퍼지 매칭
- **최소 운영 오버헤드** = 완전 관리형 ML 서비스

### 3. AWS Glue Crawler - 단일 테이블 생성 보장

**문제 시나리오**: S3 객체들에 대해 Glue Crawler 실행 시 여러 테이블이 생성됨, 하나의 테이블만 생성되도록 해야 함

**정답: 1) 객체 형식, 압축 타입, 스키마 통일 + 2) S3 객체 이름 prefix 구조 일관성**

**Glue Crawler의 테이블 생성 로직:**

**A. 테이블 분할 기준 이해:**

Crawler가 별도 테이블을 생성하는 경우:
1. **다른 파일 형식**: JSON, CSV, Parquet 등 혼재
2. **다른 스키마**: 컬럼 이름/타입 차이
3. **다른 압축 방식**: GZIP, SNAPPY, LZ4 등
4. **다른 prefix 패턴**: 폴더 구조 차이

**B. 해결책 1: 객체 형식/압축/스키마 통일**

**잘못된 예시 (여러 테이블 생성):**
```
s3://data-bucket/
├── data1.csv (CSV, no compression)
├── data2.json (JSON, GZIP)
├── data3.parquet (Parquet, SNAPPY)
└── data4.avro (Avro, LZ4)
```

**올바른 예시 (단일 테이블):**
```
s3://data-bucket/
├── data1.parquet (Parquet, SNAPPY)
├── data2.parquet (Parquet, SNAPPY)
├── data3.parquet (Parquet, SNAPPY)
└── data4.parquet (Parquet, SNAPPY)
```

**스키마 통일:**
- 모든 파일이 동일한 컬럼명과 데이터 타입을 가져야 함
- 예: customer_id (string), name (string), email (string) 등

**C. 해결책 2: prefix 구조 일관성**

**잘못된 prefix 구조 (여러 테이블 생성):**
```
s3://data-bucket/
├── sales/2024/data.parquet
├── marketing/2024/data.parquet
├── finance/2024/data.parquet
└── hr/2024/data.parquet
```
→ Crawler는 sales, marketing, finance, hr을 별도 테이블로 인식

**올바른 prefix 구조 (단일 테이블):**
```
s3://data-bucket/company-data/
├── year=2024/month=01/data1.parquet
├── year=2024/month=02/data2.parquet
├── year=2024/month=03/data3.parquet
└── year=2024/month=04/data4.parquet
```
→ Crawler는 company-data를 하나의 파티션된 테이블로 인식

**D. Crawler 설정 최적화:**
- **CombineCompatibleSchemas 정책**: 호환 가능한 스키마를 하나의 테이블로 결합
- **Exclusions 설정**: _SUCCESS, .DS_Store 등 불필요한 파일 제외
- **SchemaChangePolicy**: UPDATE_IN_DATABASE로 스키마 변경 자동 반영

**실제 변환 예시:**

**변환 전 (여러 테이블 문제):**
```bash
# 다양한 형식의 파일들
aws s3 cp sales_data.csv s3://bucket/data/
aws s3 cp user_data.json s3://bucket/data/  
aws s3 cp product_data.parquet s3://bucket/data/

# Crawler 결과: 3개의 별도 테이블 생성
# - data_csv (CSV 테이블)
# - data_json (JSON 테이블)  
# - data_parquet (Parquet 테이블)
```

**변환 후 (단일 테이블 해결):**
1. **형식 통일**: 모든 데이터를 Parquet으로 변환
2. **일관된 구조**: s3://bucket/unified-data/ 하위에 배치
3. **결과**: Crawler가 1개의 통합 테이블(unified_data) 생성

**추가 고려사항:**

**파티션 구조 최적화:**
```
s3://bucket/unified-data/
├── year=2024/month=01/day=01/file1.parquet
├── year=2024/month=01/day=02/file2.parquet
└── year=2024/month=01/day=03/file3.parquet
```

**스키마 진화 관리:**
- UPDATE_IN_DATABASE: 새 컬럼 자동 추가
- DEPRECATE_IN_DATABASE: 삭제된 컬럼은 deprecated 마크

시험 포인트:
- **Crawler 단일 테이블 생성** = 형식/압축/스키마 통일 + prefix 일관성
- **테이블 분할 방지** = CombineCompatibleSchemas 정책
- **일관된 데이터 구조** = 표준화된 파일 형식과 폴더 구조

### 4. SQS 메시지 손실 방지 - DLQ와 보존 기간 

**문제 시나리오**: 애플리케이션이 SQS 큐에서 메시지 소비, 가끔 다운타임 발생, 1일 후 메시지 만료/삭제로 데이터 손실

**정답: 1) 메시지 보존 기간 증가 + 2) Dead Letter Queue (DLQ) 연결**

**SQS 메시지 손실 시나리오 분석:**

**A. 현재 문제점:**
```
애플리케이션 다운타임 → 메시지 처리 중단 → 1일 후 자동 삭제 → 데이터 영구 손실
```

**메시지 라이프사이클:**
```
Producer → SQS Queue → Consumer (다운) → 1일 보존 → 자동 삭제
         ↑ 메시지 적재   ↑ 처리 실패      ↑ 만료      ↑ 영구 손실
```

**B. 해결책 1: 메시지 보존 기간 증가**

**메시지 보존 기간 설정:**
- 기본 설정: 1일 (86400초)
- 개선된 설정: 14일 (1209600초, 최대값)
- CLI: `aws sqs set-queue-attributes --attributes MessageRetentionPeriod=1209600`

**장점:**
- **복구 시간 확보**: 14일간 메시지 유지
- **다운타임 대응**: 일시적 장애 시 데이터 보존
- **비용**: 추가 비용 없음 (기본 기능)

**C. 해결책 2: Dead Letter Queue (DLQ) 연결**

**DLQ 아키텍처:**
```
Main Queue → Consumer (실패) → Retry → DLQ (최종 보관)
    ↑         ↑ maxReceiveCount   ↑      ↑ 수동 처리 대기
메시지 생성   처리 시도 횟수     재시도   장기 보존
```

**DLQ 설정 핵심:**
- **maxReceiveCount**: 3회 처리 실패 후 DLQ로 이동
- **RedrivePolicy**: 메인 큐에서 DLQ로 자동 이동 설정
- **보존 기간**: DLQ도 14일 보존 설정

**D. DLQ 처리 흐름:**
1. **메시지 처리 실패** → 메시지를 삭제하지 않음
2. **maxReceiveCount 도달** → 자동으로 DLQ로 이동  
3. **DLQ 모니터링** → CloudWatch 알람으로 실패 메시지 감지
4. **수동/자동 재처리** → DLQ에서 메인 큐로 재전송

**E. 모니터링 및 알람:**
- **DLQ 메시지 수 알람**: ApproximateNumberOfMessages > 1일 때 SNS 알림
- **메인 큐 지연 알람**: ApproximateAgeOfOldestMessage > 30분일 때 알림

**데이터 손실 방지 효과:**

**변경 전:**
```
다운타임 1일 → 메시지 만료 → 100% 데이터 손실
```

**변경 후:**
```
다운타임 1일 → 메시지 보존 (14일) → 복구 후 처리 가능
처리 실패 → DLQ 보관 → 수동/자동 재처리
```

**비용 영향:**
- **메시지 보존 연장**: 추가 비용 없음
- **DLQ**: 추가 큐 비용 (메시지 수에 따라)
- **전체적으로**: 데이터 손실 비용 대비 매우 저렴

시험 포인트:
- **SQS 메시지 손실 방지** = 보존 기간 증가 + DLQ 연결
- **애플리케이션 다운타임 대응** = 14일 보존 기간
- **재처리 가능한 에러 처리** = DLQ를 통한 수동 복구

### 5. Kinesis → OpenSearch 실시간 데이터 수집 최적화

**문제 시나리오**: 실시간 음악 아티스트/곡 데이터 분석, 일일 수백만 이벤트, Kinesis Data Stream을 통한 데이터 수집, OpenSearch Service로 데이터 변환 후 수집, 최소 운영 오버헤드

**정답: Amazon Kinesis Data Firehose + AWS Lambda (변환) → OpenSearch Service**

**왜 Firehose + Lambda가 최적인가:**

**A. 완전 관리형 아키텍처:**
```
Kinesis Data Stream → Firehose (관리형) → Lambda (변환) → OpenSearch Service
        ↑                 ↑                    ↑              ↑
    실시간 스트림        자동 배치/버퍼링      데이터 변환      인덱싱/검색
```

**B. Firehose의 장점:**

**자동 배치 처리:**
- **버퍼링**: 5MB 또는 60초 간격으로 배치 처리
- **인덱스 순환**: 일일 인덱스 생성 (music-events-2024.03.01)
- **실패 처리**: 실패한 문서만 S3에 백업
- **Lambda 변환**: 데이터 변환 후 OpenSearch로 전송

**자동 에러 처리:**
- **재시도 메커니즘**: 실패한 레코드 자동 재시도
- **S3 백업**: 실패한 문서들 자동 백업
- **DLQ**: 최종 실패 레코드 별도 저장

**C. Lambda 변환 함수:**
- **Base64 디코딩**: Firehose 레코드 디코딩
- **데이터 변환**: 원시 데이터를 OpenSearch 최적화 형태로 변환
- **구조화**: 아티스트, 곡, 사용자 상호작용, 분석 데이터로 분류
- **타임스탬프 추가**: @timestamp 필드로 시간 기반 인덱싱 지원

**D. 다른 옵션들과 비교:**

**옵션 2: Logstash 파이프라인**
- ❌ 인프라 관리: Logstash 서버 운영 필요
- ❌ 확장성: 수동 스케일링
- ❌ 비용: EC2 인스턴스 상시 운영

**옵션 3: Lambda + Kinesis Agent**
- ❌ 복잡한 구현: 배치, 재시도, 에러 처리 직접 구현
- ❌ 관리 오버헤드: 모든 로직을 직접 관리

**옵션 4: KCL (Kinesis Client Library)**
- ❌ 애플리케이션 관리: KCL 앱 운영 필요
- ❌ 인프라: 서버 또는 컨테이너 필요

**E. Firehose + Lambda의 운영 우위:**

**완전 서버리스:**
- ✅ **인프라 관리**: 제로 (AWS 완전 관리)
- ✅ **자동 확장**: 트래픽에 따라 자동 스케일링
- ✅ **가용성**: AWS 관리형 서비스의 고가용성

**비용 효율성:**
- 일일 500만 이벤트 기준: 월 약 $146 (Firehose $145 + Lambda $1)
- 기존 OpenSearch 클러스터 활용으로 추가 비용 없음

**모니터링과 관찰성:**
- DeliveryToElasticsearch.Records: 전송 성공 레코드 수
- DeliveryToElasticsearch.Success: 전송 성공률
- ProcessingLambda.Duration/Errors: Lambda 변환 성능

**F. 핵심 설정 요소:**
- **IndexRotationPeriod**: OneDay (일일 인덱스 생성)
- **BufferingHints**: 5MB 또는 60초 간격
- **S3BackupMode**: FailedDocumentsOnly (실패한 문서만 백업)
- **ProcessingConfiguration**: Lambda 변환 함수 연결

시험 포인트:
- **실시간 스트림 → OpenSearch** = Firehose + Lambda 변환
- **최소 운영 오버헤드** = 완전 관리형 서비스 조합
- **대용량 처리 + 변환** = Firehose 네이티브 배치 + Lambda 변환