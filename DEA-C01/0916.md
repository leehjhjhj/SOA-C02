## 0916 (DEA-C01 Review)

### 1. S3 Dual-Layer Server-Side Encryption - DSSE-KMS

**문제 시나리오**: 규정 준수를 위해 S3 버킷에 업로드되는 파일에 두 개 레이어의 서버사이드 암호화를 적용해야 함

**정답: Use dual-layer server-side encryption with AWS KMS keys (DSSE-KMS)**

**A. DSSE-KMS란 무엇인가:**

**이중 암호화 메커니즘:**
- **첫 번째 레이어**: AES-256 암호화 (S3 기본)
- **두 번째 레이어**: KMS 키를 사용한 추가 암호화
- **독립적 키 관리**: 각 레이어가 서로 다른 키 사용

**규정 준수 요구사항 충족:**
```json
{
  "Rules": [{
    "ApplyServerSideEncryptionByDefault": {
      "SSEAlgorithm": "aws:kms-dsse",
      "KMSMasterKeyID": "arn:aws:kms:region:account:key/key-id"
    }
  }]
}
```

**B. Lambda 함수를 통한 자동 적용:**

**업로드 시 자동 암호화:**
```python
import boto3

def lambda_handler(event, context):
    s3_client = boto3.client('s3')

    # 객체에 DSSE-KMS 적용
    for record in event['Records']:
        bucket = record['s3']['bucket']['name']
        key = record['s3']['object']['key']

        # 이중 암호화 적용
        s3_client.copy_object(
            Bucket=bucket,
            Key=key,
            CopySource={'Bucket': bucket, 'Key': key},
            ServerSideEncryption='aws:kms-dsse',
            SSEKMSKeyId='arn:aws:kms:region:account:key/key-id'
        )
```

---

### 2. MySQL to Redshift CDC - AWS DMS Full Load + CDC

**문제**: 온프레미스 MySQL에서 Amazon Redshift로 근실시간 데이터 복제, 최소 개발 노력

**정답: Run a full load plus CDC task in AWS Database Migration Service (AWS DMS)**

**A. DMS CDC의 장점:**

**최소 개발 노력:**
- **코드 불필요**: GUI 기반 설정으로 완전 관리형 서비스
- **자동 스키마 변환**: MySQL → Redshift 자동 매핑
- **실시간 동기화**: Binary Log 기반 변경 데이터 캡처

**CDC 프로세스:**
```
MySQL Binary Log → DMS Replication Instance → Redshift
     ↑                     ↑                      ↑
  트랜잭션 로그           실시간 캡처              자동 적용
```

**B. DMS 설정 과정:**

**1단계 - 전체 로드 (Full Load):**
```sql
-- 초기 데이터 마이그레이션
SELECT COUNT(*) FROM source_table;  -- MySQL
COPY target_table FROM 's3://dms-temp-bucket/'; -- Redshift 적재
```

**2단계 - CDC (Change Data Capture):**
```sql
-- 실시간 변경사항 적용
INSERT INTO target_table VALUES (...); -- 신규 레코드
UPDATE target_table SET col = val WHERE id = 1; -- 업데이트
DELETE FROM target_table WHERE id = 1; -- 삭제
```

**C. 다른 솔루션 대비 장점:**

**Kinesis + Lambda vs DMS:**
- **DMS**: 설정만으로 완전 자동화
- **Kinesis**: 커스텀 개발 필요, 복잡한 오류 처리

---

### 3. Amazon Athena ACID Properties - 트랜잭션 지원

**문제**: S3 clickstream 데이터 쿼리, 파티셔닝 지원, ACID 속성 필요, 서버리스 솔루션

**정답: Amazon Athena**

**A. Athena의 ACID 지원:**

**Apache Iceberg 통합:**
- **Atomicity**: 트랜잭션 전체 성공 또는 실패
- **Consistency**: 스키마 진화 및 제약조건 유지
- **Isolation**: 동시 쿼리 간 격리 보장
- **Durability**: S3 기반 영구 저장

**ACID 쿼리 예시:**
```sql
-- 원자적 업데이트 (전체 성공 또는 롤백)
UPDATE clickstream_table
SET processed = true
WHERE event_date = '2023-09-16' AND status = 'pending';

-- 일관성 있는 읽기
SELECT COUNT(*) FROM clickstream_table
WHERE processed = true AND event_date = '2023-09-16';
```

**B. 비용 효율성:**

**서버리스 아키텍처:**
- **스캔한 데이터만 과금**: $5 per TB
- **인프라 관리 불필요**: 서버 provisioning 없음
- **자동 스케일링**: 동시 쿼리 자동 처리

**파티셔닝을 통한 비용 절약:**
```sql
-- 파티션 프루닝으로 스캔 데이터 최소화
SELECT user_id, SUM(clicks)
FROM clickstream_table
WHERE year = '2023' AND month = '09' AND day = '16'
GROUP BY user_id;
```

---

### 4. Redshift COPY Manifest File - 병렬 로딩 최적화

**문제**: 여러 데이터 파일을 개별 COPY 명령으로 로드하여 시간이 오래 걸림, 비용 증가 없이 속도 향상

**정답: Create a manifest file that contains the data file locations**

**A. Manifest File의 이점:**

**병렬 처리 최적화:**
- **단일 COPY 명령**: 여러 파일을 한 번에 처리
- **자동 분산**: 클러스터 노드별 작업 분산
- **연결 재사용**: 단일 연결로 모든 파일 처리

**Manifest File 구조:**
```json
{
  "entries": [
    {"url": "s3://bucket/data/file1.csv", "mandatory": true},
    {"url": "s3://bucket/data/file2.csv", "mandatory": true},
    {"url": "s3://bucket/data/file3.csv", "mandatory": true}
  ]
}
```

**B. COPY 명령 최적화:**

**기존 방식 (느림):**
```sql
-- 각 파일당 개별 연결 및 처리
COPY table1 FROM 's3://bucket/file1.csv' IAM_ROLE 'role';
COPY table1 FROM 's3://bucket/file2.csv' IAM_ROLE 'role';
COPY table1 FROM 's3://bucket/file3.csv' IAM_ROLE 'role';
```

**최적화된 방식 (빠름):**
```sql
-- 단일 명령으로 병렬 처리
COPY table1 FROM 's3://bucket/manifest.json'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
MANIFEST;
```

**C. 성능 향상 원리:**

**병렬 슬라이스 활용:**
- **슬라이스별 분산**: 각 compute node의 slice가 병렬 처리
- **I/O 최적화**: 동시 네트워크 연결로 throughput 증대
- **메모리 효율**: 단일 COPY 작업으로 메모리 사용량 최적화

---

### 5. Transfer Family TLS 1.2 Security Policy

**문제**: Transfer Family 서버 보안 정책에서 최소 TLS 1.2 프로토콜 버전 지정

**정답: Update the security policy to specify minimum protocol version of TLS 1.2**

**A. TLS 1.2 보안 강화 이유:**

**보안 취약점 해결:**
- **TLS 1.0/1.1**: 알려진 보안 취약점 (POODLE, BEAST 공격)
- **TLS 1.2**: 강화된 암호화 알고리즘 (AES-GCM, ChaCha20)
- **규정 준수**: PCI DSS, FIPS 140-2 요구사항

**Security Policy 설정:**
```json
{
  "SecurityPolicyName": "TransferSecurityPolicy-TLS-1-2-2023",
  "Protocols": ["SFTP", "FTPS"],
  "TlsOptions": {
    "MinimumVersion": "TLSv1.2"
  },
  "CipherSuites": [
    "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256"
  ]
}
```

**B. 왜 Security Policy를 업데이트하는가:**

**규정 준수 요구사항:**
- **데이터 전송 보안**: 민감한 데이터 전송 시 강력한 암호화
- **감사 요구사항**: 보안 표준 준수 증명
- **위험 완화**: 중간자 공격 및 도청 방지

---

### 6. Redshift Single COPY Command - 최대 처리량

**문제**: 수백 개 파일을 fact table에 로드, 클러스터 리소스를 최적으로 사용하여 최대 처리량 달성

**정답: Use a single COPY command to load the data**

**A. 단일 COPY 명령이 최적인 이유:**

**클러스터 리소스 최적화:**
- **모든 노드 활용**: 클러스터의 모든 compute node 동시 사용
- **병렬 슬라이스**: 각 노드의 모든 slice 동시 처리
- **메모리 효율**: 단일 트랜잭션으로 메모리 사용량 최적화

**처리량 비교:**
```
개별 COPY (비효율):
File1 → Node1 → 완료 → File2 → Node2 → ... (순차 처리)

단일 COPY (최적):
Files[1-100] → All Nodes → 동시 병렬 처리 → 완료
```

**B. COPY 최적화 전략:**

**파일 크기 최적화:**
```sql
-- 1MB-1GB 크기 파일들을 단일 명령으로 로드
COPY fact_table FROM 's3://bucket/data/'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
DELIMITER ','
COMPUPDATE OFF
STATUPDATE OFF;
```

**압축과 인코딩:**
- **자동 압축**: GZIP으로 전송량 최소화
- **컬럼 인코딩**: 저장 공간 및 I/O 최적화

---

### 7. Amazon MWAA Task Logs - 워크플로우 실패 진단

**문제**: Amazon MWAA에서 워크플로우 실패 원인 진단을 위한 로그 유형

**정답: YourEnvironmentName-Task**

**A. MWAA 로그 유형:**

**Task 로그의 중요성:**
- **작업 실행 로그**: 각 task의 실행 상세 정보
- **에러 스택트레이스**: 실패 시점과 원인 상세 분석
- **변수 값**: task 실행 중 변수 상태 확인

**로그 유형별 용도:**
```
Airflow-Task: 개별 task 실행 로그 (가장 중요)
Airflow-Scheduler: DAG 스케줄링 로그
Airflow-Worker: Worker 노드 상태 로그
Airflow-Webserver: Web UI 접근 로그
```

**B. Task 로그 분석 방법:**

**CloudWatch에서 로그 확인:**
```bash
# 실패한 task의 로그 필터링
log_group = "/aws/amazonmwaa/YourEnvironmentName-Task"
filter_pattern = "[ERROR]"
```

**일반적인 실패 원인:**
- **의존성 에러**: 라이브러리 import 실패
- **연결 에러**: 데이터베이스/S3 연결 문제
- **데이터 에러**: 스키마 불일치, null 값 처리

---

### 8. AWS Lake Formation FindMatches - 중복 레코드 식별

**문제**: 공통 고유 식별자가 없는 레코드들 간 매칭 식별

**정답: Train and use the AWS Lake Formation FindMatches transform**

**A. FindMatches의 작동 원리:**

**머신러닝 기반 매칭:**
- **자동 학습**: 샘플 데이터로 매칭 패턴 학습
- **확률적 매칭**: 유사도 점수로 매칭 여부 판단
- **fuzzy matching**: 오타, 형식 차이 허용

**매칭 예시:**
```
Record 1: John Smith, 123 Main St, (555) 123-4567
Record 2: J. Smith, 123 Main Street, 555-123-4567
→ 매칭 확률: 95% (동일 인물로 판단)
```

**B. ETL 파이프라인 통합:**

**Glue Job에서 FindMatches 사용:**
```python
# FindMatches Transform 적용
matches = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={
        "paths": ["s3://bucket/customer-data/"]
    },
    transformation_ctx="matches"
)

# 중복 제거 및 마스터 레코드 생성
deduplicated = FindMatches.apply(
    frame=matches,
    transformation_ctx="dedup"
)
```

**C. 다른 솔루션 대비 장점:**

**수동 룰 기반 vs ML 기반:**
- **수동 룰**: 복잡한 조건문, 유지보수 어려움
- **FindMatches**: 자동 학습, 지속적 정확도 향상

---

### 9. Redshift Compound Sort Key 최적화

**문제**: Region ID, Department ID, Role ID로 구성된 compound sort key 테이블에서 최적 쿼리

**정답: A, B (Region ID 필터 + Region ID & Department ID 필터)**

**A. Compound Sort Key 작동 원리:**

**순서가 중요한 이유:**
- **첫 번째 컬럼**: Region ID로 먼저 정렬
- **두 번째 컬럼**: 같은 Region 내에서 Department ID 정렬
- **세 번째 컬럼**: 같은 Region+Department 내에서 Role ID 정렬

**최적화된 쿼리 패턴:**
```sql
-- A. 첫 번째 sort key 사용 (매우 효율적)
SELECT * FROM Employee
WHERE Region_ID = 'North America';

-- B. 첫 번째 + 두 번째 sort key 사용 (매우 효율적)
SELECT * FROM Employee
WHERE Region_ID = 'North America' AND Department_ID = 20;
```

**B. 비효율적인 쿼리 패턴:**

**Sort Key 순서 무시:**
```sql
-- 비효율: 두 번째 컬럼만 필터링
SELECT * FROM Employee WHERE Department_ID = 20;

-- 비효율: 세 번째 컬럼만 필터링
SELECT * FROM Employee WHERE Role_ID = 5;
```

**성능 차이:**
- **최적 쿼리**: Zone Map 활용으로 블록 스킵
- **비최적 쿼리**: 전체 테이블 스캔 필요

---

### 10. AWS Glue FindMatches - 고객 레코드 연결

**문제**: 서로 다른 데이터베이스의 일치하지 않는 필드명 (place_id vs location_id)으로 고객 레코드 연결

**정답: Create AWS Glue crawler + FindMatches transform**

**A. 왜 이 솔루션이 최적인가:**

**최소 운영 오버헤드:**
- **자동 스키마 발견**: Crawler가 데이터베이스 스키마 자동 분석
- **ML 기반 매칭**: 수동 매핑 룰 작성 불필요
- **지속적 학습**: 매칭 정확도 자동 개선

**FindMatches 설정:**
```python
# 매칭 규칙 자동 학습
transform_config = {
    "TransformType": "FIND_MATCHES",
    "FindMatchesParameters": {
        "PrimaryKeyColumnName": "customer_id",
        "PrecisionRecallTradeoff": 0.8,  # 정확도 우선
        "AccuracySpeedTradeoff": 0.5     # 균형 모드
    }
}
```

**B. 필드 매핑 자동화:**

**다른 필드명 자동 인식:**
```
Database 1: place_id, customer_name, phone
Database 2: location_id, cust_name, telephone
→ FindMatches가 자동으로 유사 필드 매핑 학습
```

**운영 효율성:**
- **설정 한 번**: 초기 학습 후 자동 매칭
- **확장 가능**: 새로운 데이터베이스 추가 시 쉬운 적용
- **모니터링**: Glue 콘솔에서 매칭 품질 추적

---

### 11. Redshift 제3자 IdP 인증 - Identity Provider 등록

**문제**: 제3자 IdP를 사용하여 Redshift Query Editor 인증 설정의 첫 단계

**정답: Register the third-party IdP as an identity provider from within Amazon Redshift**

**A. Redshift IdP 통합 과정:**

**1단계 - IdP 등록:**
```sql
-- Redshift에서 IdP 등록
REGISTER IDENTITY PROVIDER my_idp
TYPE 'SAML'
WITH (
  metadata_url='https://idp.company.com/metadata',
  role_arn='arn:aws:iam::account:role/RedshiftIdPRole'
);
```

**2단계 - 사용자 매핑:**
```sql
-- IdP 그룹을 Redshift 역할에 매핑
CREATE USER "engineering_team" FROM IDENTITY PROVIDER my_idp;
GRANT SELECT ON ALL TABLES IN SCHEMA public TO "engineering_team";
```

**B. 인증 플로우:**

**SAML 기반 SSO:**
```
사용자 → IdP 로그인 → SAML 토큰 → Redshift → Query Editor 접근
  ↑            ↑            ↑            ↑            ↑
브라우저    제3자 IdP    IdP 응답    토큰 검증    세션 생성
```

**보안 이점:**
- **중앙 집중식 관리**: IdP에서 사용자 관리
- **MFA 지원**: IdP의 다중 인증 활용
- **감사 추적**: 통합된 접근 로그