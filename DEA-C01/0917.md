## 0917 (DEA-C01 Review)

### 1. OpenSearch Service 데이터 수집 - Kinesis Data Firehose + Lambda

**문제 시나리오**: 미디어 회사가 음악 아티스트와 노래에 대한 실시간 데이터를 Amazon OpenSearch Service로 분석하려고 함. 하루에 수백만 개의 새로운 데이터 이벤트를 Kinesis 데이터 스트림을 통해 수집하고, 데이터를 변환한 후 OpenSearch에 수집해야 함.

**정답: A. Use Amazon Kinesis Data Firehose and an AWS Lambda function to transform the data and deliver the transformed data to OpenSearch Service**

**A. 왜 Kinesis Data Firehose + Lambda가 최적인가:**

**최소 운영 오버헤드:**
- **완전 관리형**: 서버 관리 불필요, 자동 스케일링
- **기본 통합**: OpenSearch Service로 직접 전송 지원
- **내장 변환**: Lambda를 통한 간단한 데이터 변환

**아키텍처:**
```
Kinesis Data Stream → Kinesis Data Firehose → Lambda 변환 → OpenSearch Service
        ↑                    ↑                  ↑              ↑
    실시간 수집         배치 처리/버퍼링       데이터 변환     최종 저장
```

**B. Lambda 변환 함수 예시:**

```python
import json
import base64

def lambda_handler(event, context):
    output = []

    for record in event['records']:
        # 입력 데이터 디코딩
        payload = json.loads(base64.b64decode(record['data']))

        # 데이터 변환 로직
        transformed_data = {
            'artist_name': payload.get('artist', '').title(),
            'song_title': payload.get('song', '').title(),
            'play_count': int(payload.get('plays', 0)),
            'timestamp': payload.get('timestamp'),
            'genre': payload.get('genre', 'Unknown').lower()
        }

        # 출력 레코드 생성
        output_record = {
            'recordId': record['recordId'],
            'result': 'Ok',
            'data': base64.b64encode(
                json.dumps(transformed_data).encode('utf-8')
            ).decode('utf-8')
        }
        output.append(output_record)

    return {'records': output}
```

**C. 다른 솔루션들이 부적절한 이유:**

**B. Logstash Pipeline:**
- **높은 운영 오버헤드**: EC2에서 Logstash 서버 관리 필요
- **복잡한 설정**: Elasticsearch 플러그인 설정 및 관리

**C. Lambda + Kinesis Agent:**
- **잘못된 아키텍처**: Kinesis Agent는 데이터 수집 도구, 변환 도구가 아님
- **복잡성 증가**: 불필요한 추가 컴포넌트

**D. Kinesis Client Library (KCL):**
- **높은 개발 복잡도**: 커스텀 애플리케이션 개발 필요
- **인프라 관리**: EC2 인스턴스에서 KCL 애플리케이션 실행 필요

---

### 2. AWS Glue Data Quality Rules 대량 편집 - API 자동화

**문제**: 데이터 엔지니어가 1,000개의 AWS Glue Data Catalog 테이블에 데이터 품질 규칙을 구현했는데, 비즈니스 요구사항 변경으로 인해 모든 데이터 품질 규칙을 편집해야 함.

**정답: Create an AWS Lambda function that makes an API call to AWS Glue Data Quality to make the edits**

**A. 왜 콘솔에서는 불가능한가:**

**대량 작업의 한계:**
- **1,000개 테이블**: 수동 편집 시 수백 시간 소요
- **일관성 문제**: 수동 작업 시 실수 가능성 높음
- **버전 관리**: 변경 이력 추적 어려움

**B. Lambda 기반 자동화 솔루션:**

**배치 처리 아키텍처:**
```python
import boto3
import json

def lambda_handler(event, context):
    glue_client = boto3.client('glue')

    # 모든 데이터베이스 및 테이블 조회
    databases = glue_client.get_databases()

    for database in databases['DatabaseList']:
        db_name = database['Name']

        # 데이터베이스 내 모든 테이블 조회
        tables = glue_client.get_tables(DatabaseName=db_name)

        for table in tables['TableList']:
            table_name = table['Name']

            # 기존 데이터 품질 규칙 조회
            try:
                ruleset = glue_client.get_data_quality_ruleset(
                    Name=f"{db_name}_{table_name}_rules"
                )

                # 규칙 업데이트
                updated_rules = update_quality_rules(ruleset['Ruleset'])

                # 새 규칙 적용
                glue_client.update_data_quality_ruleset(
                    Name=f"{db_name}_{table_name}_rules",
                    Description="Updated rules for new business requirements",
                    Ruleset=updated_rules
                )

                print(f"Updated rules for {db_name}.{table_name}")

            except Exception as e:
                print(f"Error updating {db_name}.{table_name}: {str(e)}")

    return {
        'statusCode': 200,
        'body': json.dumps('Data quality rules updated successfully')
    }

def update_quality_rules(current_rules):
    # 비즈니스 요구사항에 따른 규칙 수정 로직
    updated_rules = current_rules.replace(
        'completeness "column_name" > 0.95',  # 기존 규칙
        'completeness "column_name" > 0.98'   # 새로운 요구사항
    )

    # 추가 규칙 변경사항 적용
    updated_rules += '\nuniqueness "primary_key" = 1.0'

    return updated_rules
```

**C. API 기반 접근의 장점:**

**확장성과 일관성:**
- **병렬 처리**: 여러 테이블 동시 업데이트
- **트랜잭션 보장**: 실패 시 롤백 가능
- **감사 추적**: CloudTrail을 통한 모든 변경사항 기록

**운영 효율성:**
- **재사용 가능**: 향후 유사한 변경사항에 재활용
- **스케줄링**: EventBridge로 정기적 업데이트 자동화
- **모니터링**: CloudWatch를 통한 실행 상태 추적

---

### 3. AWS Glue Dynamic Frame File Grouping - 소형 파일 처리 최적화

**문제**: 전 세계 테스트 시설에서 수백만 개의 1KB JSON 파일을 받아 Parquet으로 변환하여 Redshift에 로드하는데, 테스트 시설 증가로 인해 처리 시간이 늘어나고 있음.

**정답: Use the AWS Glue dynamic frame file-grouping option to ingest the raw input files**

**A. Small File Problem과 File Grouping:**

**소형 파일 문제점:**
- **과도한 메타데이터**: 파일당 최소 블록 크기 할당으로 저장 공간 낭비
- **느린 처리**: 파일 오픈/클로즈 오버헤드가 실제 처리 시간보다 많음
- **메모리 압박**: 수백만 개 파일 처리 시 메모리 부족

**File Grouping 솔루션:**
```python
# Glue Job에서 file grouping 옵션 설정
from awsglue.dynamicframe import DynamicFrame
from awsglue.context import GlueContext

# 소형 파일들을 논리적으로 그룹화
datasource = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={
        "paths": ["s3://test-results-bucket/"],
        "recurse": True,
        "groupFiles": "inPartition",  # 파티션별 파일 그룹화
        "groupSize": "134217728"      # 128MB 단위로 그룹화
    },
    format="json",
    transformation_ctx="grouped_source"
)
```

**B. 처리 성능 향상 원리:**

**메모리 최적화:**
```
기존 방식:
File1(1KB) → Read → Process → File2(1KB) → Read → Process → ...
↑ 파일당 64MB 블록 할당, 메모리 비효율

File Grouping:
[File1+File2+...FileN](128MB) → Read → Batch Process → Output
↑ 최적 크기로 배치 처리, 메모리 효율적
```

**처리량 비교:**
- **개별 처리**: 1백만 파일 × 0.1초/파일 = 27.8시간
- **그룹 처리**: 1백만 파일 ÷ 128개/그룹 × 1초/그룹 = 2.2시간

**C. Parquet 변환 최적화:**

**컬럼형 저장 이점:**
```python
# 파일 그룹화 후 Parquet 변환
parquet_frame = datasource.resolveChoice(specs=[
    ('test_id', 'cast:long'),
    ('timestamp', 'cast:timestamp'),
    ('result_data', 'cast:string')
])

# 압축과 함께 Parquet으로 저장
glueContext.write_dynamic_frame.from_options(
    frame=parquet_frame,
    connection_type="s3",
    connection_options={
        "path": "s3://processed-bucket/parquet/",
        "compression": "gzip"
    },
    format="parquet"
)
```

**저장 효율성:**
- **압축률**: JSON 대비 80-90% 저장 공간 절약
- **쿼리 성능**: 컬럼형 구조로 Redshift 쿼리 속도 향상

---

### 4. Oracle to S3 Data Lake - AWS DMS vs AWS Glue 비교

**문제**: 회사가 데이터센터의 Oracle 데이터베이스 70개 테이블을 매일 S3 데이터 레이크로 수집해야 함. 모든 테이블에는 기본키가 있고 소스 시스템에서 데이터가 변경될 수 있음.

**정답: C. AWS DMS for ongoing replication (Oracle → S3, Parquet format)**

**A. 왜 AWS DMS가 최적인가:**

**CDC(Change Data Capture) 지원:**
- **실시간 변경 추적**: Oracle Redo Log 기반 변경사항 자동 감지
- **증분 동기화**: 변경된 데이터만 전송으로 네트워크 효율성
- **Parquet 형식 지원**: 타겟에서 직접 Parquet 변환 가능

**DMS 설정 예시:**
```json
{
  "target-endpoint": {
    "endpoint-type": "target",
    "engine-name": "s3",
    "s3-settings": {
      "bucket-name": "data-lake-bucket",
      "compression-type": "gzip",
      "data-format": "parquet",
      "parquet-version": "parquet-2-0"
    }
  },
  "replication-task": {
    "migration-type": "full-load-and-cdc",
    "target-table-preparation-mode": "truncate-before-load"
  }
}
```

**B. 왜 AWS Glue Bookmark는 부적절한가:**

**Bookmark의 한계:**
- **배치 처리**: 실시간 변경사항 감지 불가
- **전체 테이블 스캔**: 변경된 레코드만 식별하기 어려움
- **Oracle 연결**: JDBC 연결 오버헤드로 성능 제약

**성능 비교:**
```
DMS CDC:
Oracle Redo Log → 변경 레코드만 추출 → S3 (효율적)

Glue Bookmark:
전체 테이블 스캔 → 타임스탬프 비교 → 변경 레코드 식별 (비효율적)
```

**C. DMS의 운영상 이점:**

**최소 노력 (Least Effort):**
- **설정 기반**: 코딩 없이 GUI로 복제 작업 설정
- **자동 스키마 변환**: Oracle → Parquet 자동 매핑
- **모니터링**: 내장 메트릭스로 복제 상태 추적

**고가용성:**
- **Multi-AZ 지원**: 복제 인스턴스 장애 대응
- **자동 복구**: 네트워크 단절 시 자동 재시작
- **백프레셰 방지**: 소스 DB 부하 최소화

---

### 5. Kinesis Data Streams 처리량 최적화 - KPL (Kinesis Producer Library)

**문제**: 운송회사가 차량 위치 추적을 위해 10바이트 크기 위치 기록을 초당 최대 10,000개까지 받음. 네트워크 연결 불안정으로 몇 분 지연은 허용되며, Kinesis Data Streams의 샤드 처리량 효율성을 최대화해야 함.

**정답: B. Kinesis Producer Library (KPL)**

**A. KPL의 처리량 최적화 기능:**

**배치 처리 및 압축:**
- **레코드 집계**: 여러 작은 레코드를 하나의 Kinesis 레코드로 결합
- **압축**: gzip 압축으로 네트워크 대역폭 절약
- **버퍼링**: 지연 허용 시간 내에서 배치 크기 최적화

**KPL 설정 예시:**
```java
KinesisProducerConfiguration config = new KinesisProducerConfiguration()
    .setRecordMaxBufferedTime(1000)        // 1초 버퍼링
    .setAggregationMaxCount(4294967295L)   // 최대 집계 레코드 수
    .setAggregationMaxSize(1024 * 1024)    // 1MB 최대 집계 크기
    .setCompressionType("gzip")            // 압축 활성화
    .setCredentialsProvider(credentialsProvider)
    .setRegion("us-east-1");

KinesisProducer producer = new KinesisProducer(config);

// 위치 데이터 전송
for (LocationRecord location : locationData) {
    ByteBuffer data = ByteBuffer.wrap(location.toJson().getBytes());
    producer.addUserRecord(
        "vehicle-location-stream",
        location.getVehicleId(),  // 파티션 키
        data
    );
}
```

**B. 샤드 처리량 효율성:**

**집계를 통한 처리량 증대:**
```
기존 방식 (개별 전송):
10,000 records/sec × 10 bytes = 100KB/sec
→ 10,000 API 호출/sec (비효율적)

KPL 집계:
10,000 records → 100 aggregated records/sec × 1KB = 100KB/sec
→ 100 API 호출/sec (효율적)
```

**샤드 한계 최적화:**
- **1MB/sec 제한**: 압축을 통해 더 많은 데이터 전송
- **1,000 records/sec 제한**: 집계를 통해 실질적으로 더 많은 레코드 처리

**C. 네트워크 불안정 환경 대응:**

**재시도 및 복원력:**
```java
// 네트워크 실패 시 자동 재시도
config.setRetryingMaxRetries(10)
      .setRetryingMaxDelay(30000)  // 최대 30초 대기
      .setRetryingInitialDelay(100); // 초기 100ms 대기

// 비동기 처리로 성능 최적화
producer.addUserRecord(streamName, partitionKey, data)
    .addCallback(new FutureCallback<UserRecordResult>() {
        @Override
        public void onSuccess(UserRecordResult result) {
            log.info("Record sent successfully");
        }

        @Override
        public void onFailure(Throwable t) {
            log.error("Failed to send record: " + t.getMessage());
            // 실패한 레코드 재처리 로직
        }
    });
```

**다른 솔루션 대비 장점:**
- **Kinesis Agent**: 파일 기반 수집, 실시간 스트림에 부적합
- **AWS SDK**: 개별 API 호출로 비효율적, 처리량 제한
- **Direct API**: 배치 처리 없이 높은 API 호출 빈도

---

### 6. Kinesis Enhanced Fan-out - 전용 처리량 보장

**문제**: 모바일 게임 회사가 게임 앱에서 약 20KB 크기의 데이터 레코드를 수집하고, 3개의 내부 소비자에게 데이터를 제공해야 함. 각 디바이스에서 최적 처리량을 달성하고, 각 내부 소비자별로 전용 처리량이 필요함.

**정답: A. Configure the mobile app to call the PutRecords API operation to send data to Amazon Kinesis Data Streams. Use the enhanced fan-out feature with a stream for each internal consumer**

**A. Enhanced Fan-out의 필요성:**

**전용 처리량 보장:**
- **기본 모드**: 모든 소비자가 2MB/sec 샤드 처리량을 공유
- **Enhanced Fan-out**: 각 소비자별로 2MB/sec 전용 처리량 제공

**처리량 비교:**
```
일반 Kinesis Consumer (공유 모드):
Shard: 2MB/sec ÷ 3 consumers = 0.67MB/sec per consumer

Enhanced Fan-out (전용 모드):
Consumer 1: 2MB/sec (전용)
Consumer 2: 2MB/sec (전용)
Consumer 3: 2MB/sec (전용)
```

**B. Enhanced Fan-out 설정:**

**스트림 소비자 등록:**
```python
import boto3

kinesis_client = boto3.client('kinesis')

# 각 내부 소비자별로 Enhanced Fan-out 등록
consumers = ['analytics-consumer', 'ml-consumer', 'monitoring-consumer']

for consumer_name in consumers:
    response = kinesis_client.register_stream_consumer(
        StreamARN='arn:aws:kinesis:region:account:stream/game-data-stream',
        ConsumerName=consumer_name
    )
    print(f"Registered consumer: {consumer_name}")
```

**실시간 데이터 소비:**
```python
import boto3

def consume_with_enhanced_fanout(consumer_arn):
    kinesis_client = boto3.client('kinesis')

    # Enhanced Fan-out으로 실시간 구독
    response = kinesis_client.subscribe_to_shard(
        ConsumerARN=consumer_arn,
        ShardId='shardId-000000000000',
        StartingPosition={
            'Type': 'LATEST'
        }
    )

    # 이벤트 스트림 처리
    event_stream = response['EventStream']
    for event in event_stream:
        if 'SubscribeToShardEvent' in event:
            records = event['SubscribeToShardEvent']['Records']

            for record in records:
                # 게임 데이터 처리 로직
                process_game_data(record['Data'])
```

**C. PutRecords API의 최적 처리량:**

**배치 전송 최적화:**
```python
import boto3
import json

def send_game_data_batch(kinesis_client, stream_name, game_events):
    records = []

    for event in game_events:
        record = {
            'Data': json.dumps(event),
            'PartitionKey': event['player_id']  # 플레이어별 파티셔닝
        }
        records.append(record)

        # 500개 레코드 또는 5MB마다 배치 전송
        if len(records) >= 500:
            response = kinesis_client.put_records(
                Records=records,
                StreamName=stream_name
            )

            # 실패한 레코드 재시도 처리
            failed_records = []
            for i, result in enumerate(response['Records']):
                if 'ErrorCode' in result:
                    failed_records.append(records[i])

            records = failed_records  # 실패한 레코드만 재시도

    # 남은 레코드 전송
    if records:
        kinesis_client.put_records(Records=records, StreamName=stream_name)
```

**디바이스별 최적 처리량:**
- **배치 크기**: 20KB × 25레코드 = 500KB per batch (최적)
- **전송 빈도**: 높은 처리량을 위한 비동기 배치 전송
- **파티셔닝**: 플레이어 ID 기반으로 고른 분산

---

### 7. Amazon Redshift VPC 보안 - Enhanced VPC Routing + Direct Connect

**문제**: 금융회사가 Amazon Redshift를 데이터 웨어하우스로 사용하며, 인증된 제3자 데이터 제공업체로부터 데이터를 받음. 규정 준수를 위해 회사 AWS 환경 외부에서 데이터에 접근할 수 없도록 해야 함.

**정답: A. Replace with private subnet Redshift + VPC endpoint, C. Enhanced VPC routing + Direct Connect**

**A. 왜 Enhanced VPC Routing이 필요한가:**

**기본 Redshift 네트워크 동작:**
- **퍼블릭 라우팅**: S3, DynamoDB 등 AWS 서비스로 인터넷 경유
- **보안 취약점**: 데이터가 인터넷을 통과하여 외부 노출 위험

**Enhanced VPC Routing 설정:**
```sql
-- Redshift 클러스터에서 Enhanced VPC Routing 활성화
ALTER CLUSTER my-cluster
SET enhanced_vpc_routing = true;
```

**네트워크 경로 비교:**
```
기본 라우팅:
Redshift → Internet Gateway → S3 (인터넷 경유)

Enhanced VPC Routing:
Redshift → VPC Endpoint → S3 (AWS 백본 네트워크)
```

**B. Direct Connect의 역할:**

**제3자 데이터 제공업체 연결:**
- **전용 네트워크**: 인터넷을 거치지 않는 전용 연결
- **대역폭 보장**: 안정적인 데이터 전송 성능
- **규정 준수**: 데이터가 인터넷에 노출되지 않음

**Direct Connect 아키텍처:**
```
제3자 데이터 센터 → Direct Connect → VGW → VPC → Redshift
        ↑                  ↑           ↑      ↑        ↑
   데이터 소스          전용 회선    가상게이트웨이  VPC   목적지
```

**C. 완전한 보안 솔루션:**

**프라이빗 서브넷 + VPC 엔드포인트:**
```yaml
# CloudFormation 템플릿 예시
RedshiftCluster:
  Type: AWS::Redshift::Cluster
  Properties:
    ClusterSubnetGroupName: !Ref PrivateSubnetGroup
    VpcSecurityGroupIds:
      - !Ref RedshiftSecurityGroup
    PubliclyAccessible: false
    EnhancedVpcRouting: true

S3VPCEndpoint:
  Type: AWS::EC2::VPCEndpoint
  Properties:
    VpcId: !Ref VPC
    ServiceName: com.amazonaws.region.s3
    VpcEndpointType: Gateway
    RouteTableIds:
      - !Ref PrivateRouteTable

RedshiftVPCEndpoint:
  Type: AWS::EC2::VPCEndpoint
  Properties:
    VpcId: !Ref VPC
    ServiceName: com.amazonaws.region.redshift
    VpcEndpointType: Interface
    SubnetIds:
      - !Ref PrivateSubnet
```

**보안 정책 적용:**
```json
{
  "SecurityGroupRules": [
    {
      "IpPermissions": [
        {
          "IpProtocol": "tcp",
          "FromPort": 5439,
          "ToPort": 5439,
          "UserIdGroupPairs": [
            {
              "GroupId": "sg-authorized-apps-only",
              "Description": "Internal applications only"
            }
          ]
        }
      ]
    }
  ]
}
```

**규정 준수 보장:**
- **데이터 격리**: 모든 데이터 트래픽이 AWS 내부 네트워크만 사용
- **접근 제어**: VPC 내부 리소스만 Redshift 접근 가능
- **감사 추적**: VPC Flow Logs로 모든 네트워크 활동 기록

---

### 8. Data Mesh Lake Formation - Redshift Data Share

**문제**: 중앙 거버넌스 계정을 가진 데이터 메시에서 Redshift Serverless 테이블 그룹으로 새 데이터 제품 생성. 마케팅 팀과 컴플라이언스 팀에게 서로 다른 컬럼 서브셋으로 공유해야 함.

**정답: B. Create an Amazon Redshift data share + D. Share to Lake Formation catalog**

**A. Redshift Data Share의 역할:**

**테이블 수준 공유:**
- **라이브 데이터**: 복사 없이 실시간 데이터 접근
- **세분화된 권한**: 테이블별, 컬럼별 접근 제어
- **크로스 계정**: 여러 AWS 계정 간 데이터 공유

**Data Share 생성:**
```sql
-- 중앙 거버넌스 계정에서 Data Share 생성
CREATE DATASHARE marketing_data_share;

-- 마케팅 팀용 테이블과 특정 컬럼만 공유
ALTER DATASHARE marketing_data_share
ADD TABLE customer_data (customer_id, name, purchase_history);

ALTER DATASHARE marketing_data_share
ADD TABLE product_data (product_id, category, price);

-- 컴플라이언스 팀용 별도 Data Share
CREATE DATASHARE compliance_data_share;

ALTER DATASHARE compliance_data_share
ADD TABLE customer_data (customer_id, ssn, compliance_flags);

ALTER DATASHARE compliance_data_share
ADD TABLE audit_log (timestamp, user_id, action);
```

**B. Lake Formation 통합:**

**중앙집중식 거버넌스:**
- **통합 카탈로그**: 모든 데이터 자산을 Lake Formation에서 관리
- **권한 관리**: Fine-grained access control
- **데이터 검색**: 통합된 메타데이터 카탈로그

**Lake Formation 등록:**
```python
import boto3

lakeformation_client = boto3.client('lakeformation')
redshift_client = boto3.client('redshift-data')

# Data Share를 Lake Formation에 등록
def register_datashare_with_lakeformation(datashare_arn, database_name):
    response = lakeformation_client.register_resource(
        ResourceArn=datashare_arn,
        UseServiceLinkedRole=True
    )

    # Lake Formation 데이터베이스 생성
    lakeformation_client.create_database(
        DatabaseInput={
            'Name': database_name,
            'Description': 'Redshift Data Share via Lake Formation'
        }
    )

    return response

# 마케팅 팀 권한 부여
lakeformation_client.grant_permissions(
    Principal={'DataLakePrincipalIdentifier': 'arn:aws:iam::marketing-account:root'},
    Resource={
        'Database': {'Name': 'marketing_datashare_db'}
    },
    Permissions=['SELECT'],
    PermissionsWithGrantOption=[]
)

# 컴플라이언스 팀 권한 부여 (다른 컬럼 접근)
lakeformation_client.grant_permissions(
    Principal={'DataLakePrincipalIdentifier': 'arn:aws:iam::compliance-account:root'},
    Resource={
        'Database': {'Name': 'compliance_datashare_db'}
    },
    Permissions=['SELECT'],
    PermissionsWithGrantOption=[]
)
```

**C. 컬럼 수준 보안:**

**마케팅 팀 뷰:**
```sql
-- 마케팅 팀이 볼 수 있는 컬럼
CREATE VIEW marketing_customer_view AS
SELECT
    customer_id,
    first_name,
    last_name,
    email,
    purchase_amount,
    product_preferences
FROM shared_datashare.customer_data;
```

**컴플라이언스 팀 뷰:**
```sql
-- 컴플라이언스 팀이 볼 수 있는 컬럼
CREATE VIEW compliance_customer_view AS
SELECT
    customer_id,
    ssn,
    tax_id,
    regulatory_status,
    kyc_completion_date,
    risk_score
FROM shared_datashare.customer_data;
```

**D. Data Mesh 아키텍처 이점:**

**분산 데이터 소유권:**
- **도메인별 책임**: 각 팀이 자신의 데이터 제품 관리
- **중앙 거버넌스**: Lake Formation을 통한 통합 정책 관리
- **셀프 서비스**: 승인된 사용자의 자율적 데이터 접근

**확장성과 보안:**
- **실시간 접근**: 데이터 복사 없이 최신 데이터 제공
- **감사 가능**: 모든 데이터 접근 기록 추적
- **규정 준수**: 컬럼 수준 접근 제어로 데이터 프라이버시 보장

---

이 문서는 AWS 데이터 엔지니어링 서비스들의 실제 사용 사례와 최적화 전략을 다루고 있습니다. 각 솔루션은 운영 효율성, 비용 최적화, 보안 요구사항을 모두 고려한 실무적 접근 방법을 제시합니다.