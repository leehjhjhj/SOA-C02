## 0904 (~page 22)

### 1. S3 Object Lambda - 동적 PII 필터링

**문제 시나리오**: 전자상거래 애플리케이션(PII 포함) + 내부 분석 애플리케이션(PII 불필요), 동일 S3 데이터 사용

**해결책: S3 Object Lambda**

S3 Object Lambda 특징:
- **동적 변환**: 요청 시점에 데이터 변환/필터링
- **애플리케이션별 처리**: 요청 소스에 따른 다른 처리 로직
- **원본 보존**: S3 원본 데이터 변경 없이 뷰 제공
- **서버리스**: Lambda 기반 자동 스케일링

아키텍처:
```
S3 버킷 (원본 데이터)
    ↓
S3 Object Lambda Endpoint
    ↓
Lambda 함수 (필터링 로직)
    ↓
┌─────────────┬─────────────────┐
│ 전자상거래   │ 내부 분석        │
│ (PII 포함)  │ (PII 제거됨)    │
└─────────────┴─────────────────┘
```

Lambda 함수 예시:
```python
def lambda_handler(event, context):
    # 요청 컨텍스트 확인
    user_identity = event['userIdentity']
    
    # S3에서 원본 객체 가져오기
    response = s3.get_object(
        Bucket=event['s3Bucket'],
        Key=event['s3Key']
    )
    
    data = json.loads(response['Body'].read())
    
    # PII 필터링 로직
    if 'analytics' in user_identity.get('type', ''):
        # 내부 분석용: PII 제거
        filtered_data = redact_pii(data)
    else:
        # 전자상거래용: 원본 데이터
        filtered_data = data
    
    return {
        'statusCode': 200,
        'body': json.dumps(filtered_data)
    }
```

시험 포인트:
- 동적 데이터 필터링 + 애플리케이션별 액세스 = S3 Object Lambda

### 2. CloudWatch Logs → Splunk 실시간 전송

**문제 시나리오**: VPC Flow Logs → CloudWatch Logs → Splunk, 실시간 전송, 최소 운영 오버헤드

**해결책 비교: Kinesis Data Streams vs Kinesis Data Firehose**

**정답: Kinesis Data Firehose**

**Kinesis Data Firehose 장점:**
- **완전 관리형**: 서버리스, 자동 스케일링
- **Splunk 네이티브 지원**: 사전 구축된 Splunk 커넥터
- **버퍼링**: 배치 처리로 효율적 전송
- **압축**: 자동 데이터 압축으로 비용 절약
- **에러 처리**: 실패 시 S3 백업

**Kinesis Data Streams 단점:**
- **수동 관리**: 샤드 관리 및 스케일링 필요
- **추가 처리**: Splunk 전송을 위한 별도 애플리케이션 필요
- **복잡성**: 높은 운영 오버헤드

아키텍처:
```
VPC Flow Logs → CloudWatch Logs → Subscription Filter → Kinesis Data Firehose → Splunk
```

설정:
```json
{
  "DeliveryStreamName": "splunk-delivery-stream",
  "SplunkDestinationConfiguration": {
    "HECEndpoint": "https://splunk-hec-endpoint",
    "HECToken": "your-hec-token",
    "HECEndpointType": "Event",
    "ProcessingConfiguration": {
      "Enabled": true,
      "Processors": [
        {
          "Type": "Lambda",
          "Parameters": [
            {
              "ParameterName": "LambdaArn",
              "ParameterValue": "arn:aws:lambda:region:account:function:log-processor"
            }
          ]
        }
      ]
    }
  }
}
```

시험 포인트:
- 실시간 로그 전송 + 최소 운영 오버헤드 = Kinesis Data Firehose

### 3. AWS Glue 구성 요소 관계

**Glue 종속성 및 워크플로우:**

```
Data Catalog (메타데이터 저장소)
    ↑
Crawler (스키마 자동 감지) → Job (ETL 처리)
    ↓                        ↓
Workflow (오케스트레이션)
    ↓
Trigger (실행 조건)
```

구성 요소별 역할:

**1. Data Catalog**
- 메타데이터 저장소
- 테이블/스키마 정의
- 파티션 정보 관리

**2. Crawler**
- 자동 스키마 감지
- Data Catalog 업데이트
- 새 파티션 발견

**3. Job**
- ETL 작업 실행
- Data Catalog 참조
- 데이터 변환/이동

**4. Workflow**
- Job과 Crawler 오케스트레이션
- 종속성 관리
- 실행 순서 제어

**5. Trigger**
- 스케줄 기반 실행
- 이벤트 기반 실행
- 조건부 실행

시험 포인트:
- Crawler → Data Catalog → Job → Workflow 순서로 종속성 이해

### 4. Glue Job Bookmarks - 증분 데이터 처리

**문제 시나리오**: Glue ETL 작업이 S3 전체 데이터 처리, 일일 증분 데이터만 처리 필요

**해결책: Job Bookmarks 활성화**

**Job Bookmarks 특징:**
- **상태 추적**: 마지막 처리 위치 기록
- **증분 처리**: 새로운 데이터만 처리
- **자동 관리**: Glue가 자동으로 상태 업데이트
- **DynamicFrame 통합**: 자동으로 새 파일만 읽기

Job Bookmarks 작동 원리:
```python
# Job Bookmarks 활성화된 Glue 작업
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job

# Job Bookmarks 설정
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# DynamicFrame 읽기 (Job Bookmarks 자동 적용)
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="my_database",
    table_name="s3_table",
    transformation_ctx="datasource"
)

# 변환 및 로드 작업...

# Job 완료 시 상태 커밋
job.commit()
```

Job Bookmarks vs 다른 방법들:

**Job Bookmarks 장점:**
- **코딩 불필요**: 설정만으로 증분 처리
- **자동 상태 관리**: Glue가 자동으로 처리 위치 추적
- **실패 복구**: 실패 지점부터 재시작

**파티션 기반 처리 단점:**
- **코딩 필요**: 날짜 기반 필터링 로직 구현
- **수동 관리**: 처리된 파티션 추적 필요

**파일명 필터링 단점:**
- **복잡한 로직**: 파일명 패턴 매칭 구현
- **에러 가능성**: 파일명 변경 시 문제 발생

설정 방법:
```json
{
  "DefaultArguments": {
    "--job-bookmark-option": "job-bookmark-enable"
  }
}
```

시험 포인트:
- 증분 데이터 처리 + 최소 코딩 노력 = Job Bookmarks

### 5. Kinesis Data Streams vs Data Firehose 상세 비교

**핵심 차이점:**

| 특성 | Data Streams | Data Firehose |
|------|--------------|---------------|
| **관리 수준** | 수동 관리 (샤드) | 완전 관리형 |
| **지연시간** | 실시간 (밀리초) | 준실시간 (60초~15분) |
| **데이터 보관** | 1~365일 보관 | 보관 없음 (즉시 전송) |
| **데이터 재처리** | 가능 (보관 기간 내) | 불가능 |
| **스케일링** | 수동 샤드 조정 | 자동 스케일링 |
| **대상** | 사용자 정의 애플리케이션 | 사전 정의된 대상만 |
| **데이터 변환** | Lambda 수동 구성 | 내장 변환 지원 |
| **비용** | 샤드 시간 + PUT 요청 | 처리된 데이터량 |
| **순서 보장** | 파티션 키 단위 보장 | 보장 없음 |
| **중복 처리** | At-least-once | 정확히 한 번 (대상에 따라) |

**데이터 보관 및 재처리:**

**Data Streams:**
```python
# 과거 데이터 재처리 가능
import boto3

kinesis = boto3.client('kinesis')

# 24시간 전 데이터부터 재처리
response = kinesis.get_shard_iterator(
    StreamName='my-stream',
    ShardId='shardId-000000000000',
    ShardIteratorType='AT_TIMESTAMP',
    Timestamp=datetime.now() - timedelta(hours=24)
)

# 저장된 레코드 순차 읽기
records = kinesis.get_records(
    ShardIterator=response['ShardIterator']
)
```

**Data Firehose:**
- 데이터 보관 없음
- 한번 전송되면 재처리 불가
- 실패 시에만 S3 에러 버킷에 백업

**버퍼링 및 배치 처리:**

**Data Streams:**
- 개별 레코드 즉시 처리
- 컨슈머가 직접 배치 구성

**Data Firehose:**
- 자동 버퍼링 (크기 또는 시간 기준)
- 버퍼 설정:
```json
{
  "BufferingHints": {
    "SizeInMBs": 5,        // 5MB 또는
    "IntervalInSeconds": 300  // 5분 중 먼저 도달하는 조건
  }
}
```

**처리량 및 확장성:**

**Data Streams:**
- 샤드당 1MB/초 또는 1,000레코드/초 (입력)
- 샤드당 2MB/초 (출력)
- 수동 샤드 분할/병합 필요

**Data Firehose:**
- 자동 확장, 제한 없음
- AWS가 자동으로 용량 관리

**대상 서비스 지원:**

**Data Streams:**
- Lambda, EC2, EMR, Kinesis Analytics
- 사용자 정의 애플리케이션
- 실시간 분석 도구

**Data Firehose:**
- S3, Redshift, Elasticsearch, Splunk
- HTTP 엔드포인트
- 사전 정의된 대상만 지원

**실시간 vs 준실시간:**

**Data Streams 실시간 처리:**
```python
# Lambda에서 실시간 처리
def lambda_handler(event, context):
    for record in event['Records']:
        # 밀리초 단위 즉시 처리
        payload = base64.b64decode(record['kinesis']['data'])
        process_immediately(payload)
```

**Data Firehose 준실시간 처리:**
```
데이터 생산 → 버퍼 (최대 15분) → 배치 전송 → 대상 서비스
```

**비용 구조:**

**Data Streams:**
- 샤드 시간: $0.015/샤드/시간
- PUT 요청: $0.014/100만 요청
- 확장 샤드 시간: $0.0036/샤드/시간

**Data Firehose:**
- 처리된 데이터: $0.029/GB
- 데이터 변환: $0.018/GB (Lambda 사용 시)
- 압축 포함, 인프라 비용 없음

**선택 기준:**

**Data Streams 선택 시:**
- 실시간 분석 필요 (밀리초 단위)
- 데이터 재처리 요구사항
- 순서 보장 필요
- 사용자 정의 컨슈머 로직
- 복잡한 스트림 처리 (집계, 윈도우 등)

**Data Firehose 선택 시:**
- 데이터 로딩/아카이빙 목적
- 완전 관리형 솔루션 선호
- S3, Redshift, Splunk 등 지원 대상 사용
- 운영 오버헤드 최소화
- 준실시간 처리로 충분 (분 단위)

**실제 시나리오 예시:**

**Data Streams 적합:**
- 금융 거래 실시간 모니터링
- IoT 센서 데이터 실시간 알람
- 게임 플레이어 활동 실시간 분석
- 클릭스트림 실시간 개인화

**Data Firehose 적합:**
- 로그 파일 S3 아카이빙
- 데이터 웨어하우스 일괄 로딩
- 검색 엔진 인덱싱
- 규정 준수를 위한 데이터 백업

시험 포인트:
- **실시간 + 재처리 + 순서보장** = Data Streams
- **준실시간 + 완전관리 + 아카이빙** = Data Firehose
- **데이터 보관 필요** = Data Streams
- **운영 오버헤드 최소화** = Data Firehose