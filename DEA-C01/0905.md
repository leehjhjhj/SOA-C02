## 0905 (~page 30)

### 1. IoT 센서 데이터 - 최소 지연시간으로 S3 전송

**문제 시나리오**: IoT 센서가 10초마다 100KB 데이터 전송, downstream 프로세스가 30초마다 S3에서 읽기

**정답: Amazon Kinesis Data Streams and call the Kinesis Client Library to deliver the data to the S3 bucket. Use a 5 second buffer interval from an application.**

**왜 Kinesis Data Streams + KCL이 정답인가:**

**최소 지연시간 달성:**
- **실시간 스트리밍**: Kinesis Data Streams는 밀리초 단위 지연시간
- **5초 버퍼링**: 애플리케이션에서 5초 간격으로 배치 처리하여 효율성과 지연시간 균형
- **직접 제어**: KCL을 통해 정확히 필요한 시점에 S3 업로드 가능

**아키텍처:**
```
IoT 센서 (100KB/10초) → Kinesis Data Streams → KCL Consumer App → S3 버킷
                          ↑ 실시간 ingestion    ↑ 5초 버퍼링
```

**KCL 애플리케이션 구현:**
```python
import boto3
from amazon_kinesis_client import KinesisConsumer
import time
from collections import deque

class IoTDataProcessor(KinesisConsumer):
    def __init__(self):
        self.s3_client = boto3.client('s3')
        self.buffer = deque()
        self.last_upload_time = time.time()
        self.buffer_interval = 5  # 5초 버퍼 간격
        
    def process_record(self, record):
        # 레코드를 버퍼에 추가
        self.buffer.append(record['data'])
        
        # 5초 간격으로 S3에 업로드
        current_time = time.time()
        if current_time - self.last_upload_time >= self.buffer_interval:
            self.flush_to_s3()
            self.last_upload_time = current_time
    
    def flush_to_s3(self):
        if self.buffer:
            # 버퍼된 데이터를 S3에 업로드
            batch_data = b''.join(self.buffer)
            self.s3_client.put_object(
                Bucket='iot-data-bucket',
                Key=f'iot-data/{int(time.time())}.json',
                Body=batch_data
            )
            self.buffer.clear()
```

**지연시간 분석:**
```
센서 데이터 → Streams (< 1초) → KCL 버퍼링 (최대 5초) → S3 업로드 (< 1초) 
= 최대 7초 지연시간
```

**결정적 요인: Firehose 버퍼 제약**

**Firehose의 치명적 한계:**
```
Firehose 최소 버퍼링 = 60초 OR 1MB (둘 중 먼저 도달하는 조건)
문제 요구사항 = 5초 버퍼 간격
→ Firehose로는 물리적으로 불가능

**현재 Firehose → S3 작동 방식**
S3 메인 대상: 최소 설정(버퍼 시간 60초 또는 버퍼 크기 1MB) 단위로만 데이터를 S3에 전달할 수 있습니다. 5초 혹은 10초마다 데이터를 넣어도 60초가 지나거나 1MB 이상이 쌓여야 파일이 S3에 생성됩니다.

Zero Buffering: 2023년 말부터 지원되었으나, 이는 일부 애플리케이션 타겟에만 해당하고, S3(특히 backup/dynamic partitioning 포함)로의 전송에는 적용되지 않습니다. S3 메인 대상은 zero buffering이 불가하므로 10초 주기의 데이터가 최소 60초 latency로 묶입니다

```

**Data Streams + KCL이 유일한 해결책:**
- **커스텀 버퍼링**: 애플리케이션에서 정확히 5초 간격 구현
- **실시간 ingestion**: 밀리초 단위 데이터 수집
- **단순 아키텍처**: 불필요한 추가 컴포넌트 없음

**다른 옵션들(D 등)의 문제:**
- Lambda, Step Functions 등 불필요한 추가 서비스
- 단순한 데이터 전달에 과도한 복잡성
- 추가 지연시간과 비용 발생

시험 포인트:
- **최소 지연시간 + 5초 버퍼링** = Kinesis Data Streams + KCL
- **실시간 데이터 처리 + 정확한 타이밍 제어** = Data Streams가 Firehose보다 우수
- **유연한 버퍼 간격 설정** = 애플리케이션 레벨에서 제어 가능

### 2. AWS Glue Data Quality - 사용자 정의 룰셋

**문제 시나리오**: 데이터 품질 검사를 위한 특정 룰 정의 필요

**해결책: AWS Glue Data Quality Custom Rulesets**

**AWS Glue Data Quality 특징:**
- **사전 정의 룰**: 완성도, 일관성, 유효성 검사
- **사용자 정의 룰**: 비즈니스 로직에 맞는 특별 검사
- **자동 통합**: Glue ETL 작업과 네이티브 통합
- **결과 저장**: CloudWatch 메트릭 및 Data Catalog 저장

사용자 정의 룰셋 예시:
```python
# Glue Data Quality 룰셋 정의
rules = [
    "Rules = [",
    "  ColumnCount = 5",  # 컬럼 수 검증
    "  IsComplete \"customer_id\"",  # 필수 필드 완성도
    "  Uniqueness \"customer_id\" > 0.95",  # 고유성 검증
    "  Mean \"age\" between 18 and 100",  # 값 범위 검증
    "  ColumnValues \"status\" in [\"active\", \"inactive\"]",  # 값 제약
    "  CustomSQL \"SELECT COUNT(*) FROM primary WHERE created_date > CURRENT_DATE - 1\" > 0"  # 사용자 정의 SQL
    "]"
]
```

ETL 작업과 통합:
```python
# Glue Job에서 Data Quality 사용
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue import DynamicFrame

job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# 데이터 로드
df = glueContext.create_dynamic_frame.from_catalog(
    database="my_db", 
    table_name="customer_data"
)

# Data Quality 검사 실행
dq_ruleset = """
Rules = [
  IsComplete "customer_id",
  Uniqueness "email" > 0.95,
  ColumnValues "age" between 18 and 120
]
"""

# 품질 검사 및 결과 확인
quality_result = glueContext.run_data_quality_evaluation(
    frame=df,
    ruleset=dq_ruleset,
    publishing_options={
        "dataQualityEvaluationContext": "customer_validation",
        "enableDataQualityResultsPublishing": True
    }
)
```

시험 포인트:
- **특정 데이터 품질 검사** = Custom Data Quality Rulesets

### 3. Redshift 저장 프로시저 스케줄링 - 비용 효율적 방법

**문제 시나리오**: 테스트 완료된 Redshift 저장 프로시저를 일일 자동 실행, 비중요 테이블, 최대 비용 효율성

**해결책: Query Editor v2 스케줄링**

**분석:**

**Query Editor v2 스케줄링 장점:**
- **무료 서비스**: 스케줄링 기능 자체는 무료
- **서버리스**: 별도 인프라 불필요
- **간단한 설정**: GUI에서 스케줄 설정
- **Redshift 네이티브**: 추가 서비스 연동 불필요

**대안들의 비용 이슈:**

**EventBridge + Lambda:**
- Lambda 실행 비용 (매일)
- EventBridge 룰 비용
- 추가 인프라 복잡성

**AWS Step Functions:**
- 상태 전환 비용
- 오버엔지니어링 (단순 스케줄링에 과도)

**EC2 cron:**
- EC2 인스턴스 24/7 실행 비용
- 가장 비용 비효율적

Query Editor v2 설정:
```sql
-- 저장 프로시저 생성 (이미 완료)
CREATE OR REPLACE PROCEDURE process_daily_data()
AS $$
BEGIN
  -- 데이터 처리 로직
  INSERT INTO non_critical_table
  SELECT processed_data FROM source_table
  WHERE date = CURRENT_DATE - 1;
  
  COMMIT;
END;
$$ LANGUAGE plpgsql;
```

스케줄 설정:
1. Query Editor v2 접속
2. Saved queries → Create schedule
3. 매일 실행 cron 표현식: `0 2 * * *` (새벽 2시)
4. SQL: `CALL process_daily_data();`

**왜 이 방법이 가장 비용 효율적인가:**
- **제로 추가 비용**: Redshift 클러스터 이용료 외 추가 비용 없음
- **운영 오버헤드 없음**: 관리할 추가 서비스 없음
- **비중요 테이블**: 고가용성 솔루션 불필요

시험 포인트:
- **비중요 작업 + 일일 스케줄 + 최대 비용 효율성** = Query Editor v2

### 4. Lambda + EventBridge 권한 문제 해결

**문제 시나리오**: EventBridge가 Lambda 함수 호출 시 AccessDeniedException 발생

**해결책: Lambda Resource-based Policy + EventBridge IAM 역할 권한 확인**

**왜 1번 (Trust Policy)이 아닌가:**

**Trust Policy의 역할:**
- Lambda **실행 역할**이 어떤 서비스에 의해 **assume** 될 수 있는지 정의
- Lambda 함수가 **실행될 때** 어떤 권한을 가질지 결정

EventBridge → Lambda 호출 시:
```
EventBridge → Lambda 함수 호출 → Lambda 실행 역할 assume → 함수 실행
```

**실제 문제:**
1. **EventBridge → Lambda 호출 권한** (Resource-based Policy)
2. **EventBridge 자체 권한** (IAM Role)

**올바른 해결책 (2번):**

**A. Lambda Resource-based Policy 설정:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowEventBridgeInvoke",
      "Effect": "Allow",
      "Principal": {
        "Service": "events.amazonaws.com"
      },
      "Action": "lambda:InvokeFunction",
      "Resource": "arn:aws:lambda:region:account:function:my-function",
      "Condition": {
        "StringEquals": {
          "AWS:SourceArn": "arn:aws:events:region:account:rule/my-rule"
        }
      }
    }
  ]
}
```

CLI로 설정:
```bash
aws lambda add-permission \
  --function-name my-function \
  --statement-id AllowEventBridge \
  --action lambda:InvokeFunction \
  --principal events.amazonaws.com \
  --source-arn arn:aws:events:region:account:rule/my-rule
```

**B. EventBridge IAM 역할 권한:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "lambda:InvokeFunction"
      ],
      "Resource": "arn:aws:lambda:region:account:function:my-function"
    }
  ]
}
```

**권한 흐름 이해:**
```
EventBridge Rule (IAM Role) → Lambda Resource-based Policy → Lambda 함수 호출
                ↑                        ↑                    ↓
        lambda:InvokeFunction      events.amazonaws.com    실행 역할 assume
```

**Trust Policy vs Resource-based Policy:**

**Trust Policy (Lambda 실행 역할):**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "lambda.amazonaws.com"  # Lambda 서비스가 이 역할을 assume
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
```

**Resource-based Policy (Lambda 함수):**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "events.amazonaws.com"  # EventBridge가 이 함수를 호출
      },
      "Action": "lambda:InvokeFunction"
    }
  ]
}
```

**디버깅 단계:**
1. CloudTrail 로그 확인하여 어느 단계에서 실패하는지 파악
2. Lambda 함수의 Resource-based Policy 확인
3. EventBridge 룰의 IAM 역할 권한 확인

시험 포인트:
- **서비스 간 호출 권한 문제** = Resource-based Policy + 호출자 IAM 권한
- **Trust Policy ≠ Resource-based Policy** 구분 중요

### 5. DSSE-KMS (Dual-layer Server-Side Encryption)

**문제 시나리오**: 최고 수준의 데이터 암호화 보안 필요

**해결책: DSSE-KMS**

**DSSE (Dual-layer Server-Side Encryption) 란:**
- **이중 암호화**: 두 개의 독립적인 암호화 계층
- **KMS 관리**: AWS KMS로 키 관리
- **규정 준수**: 극도로 민감한 데이터용 (금융, 의료, 정부)

**DSSE 구조:**
```
원본 데이터 → 1차 암호화 (AES-256) → 2차 암호화 (AES-256) → 저장된 데이터
             ↑ KMS 키 1              ↑ KMS 키 2
```

**기존 암호화 방식과 비교:**

**SSE-S3 (단일 계층):**
```
데이터 → AES-256 암호화 → 저장
         ↑ S3 관리 키
```

**SSE-KMS (단일 계층):**
```
데이터 → AES-256 암호화 → 저장
         ↑ KMS 고객 키
```

**DSSE-KMS (이중 계층):**
```
데이터 → 1차 AES-256 → 2차 AES-256 → 저장
         ↑ KMS 키      ↑ KMS 키
```

**DSSE-KMS의 장점:**
- **방어 심화**: 한 계층 침해 시에도 추가 보호
- **독립적 키 관리**: 각 계층마다 다른 KMS 키
- **규정 준수**: FIPS 140-2 Level 3 요구사항 충족
- **감사 가능**: 각 계층별 키 사용 로그

**사용 사례:**
- **금융 데이터**: 고객 거래 정보, 신용카드 데이터
- **의료 데이터**: 환자 기록, HIPAA 준수
- **정부 데이터**: 기밀 문서, FedRAMP 요구사항
- **기업 기밀**: 특허, 영업 비밀

**S3에서 DSSE-KMS 설정:**
```bash
# S3 버킷에 DSSE-KMS 설정
aws s3api put-bucket-encryption \
  --bucket my-secure-bucket \
  --server-side-encryption-configuration '{
    "Rules": [
      {
        "ApplyServerSideEncryptionByDefault": {
          "SSEAlgorithm": "aws:kms:dsse",
          "KMSMasterKeyID": "arn:aws:kms:region:account:key/key-id"
        },
        "BucketKeyEnabled": false
      }
    ]
  }'
```

**CloudFormation 설정:**
```yaml
S3Bucket:
  Type: AWS::S3::Bucket
  Properties:
    PublicAccessBlockConfiguration:
      BlockPublicAcls: true
      BlockPublicPolicy: true
      IgnorePublicAcls: true
      RestrictPublicBuckets: true
    BucketEncryption:
      ServerSideEncryptionConfiguration:
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: aws:kms:dsse
            KMSMasterKeyID: !Ref MyKMSKey
          BucketKeyEnabled: false
```

**비용 고려사항:**
- **KMS 키 비용**: 2개 키 × $1/월
- **KMS API 호출**: 더 많은 암호화/복호화 요청
- **성능**: 이중 암호화로 인한 약간의 지연

**언제 DSSE를 사용하는가:**
- 극도로 민감한 데이터
- 규정 준수 요구사항 (FIPS, HIPAA, SOX)
- 산업별 규제 (금융, 의료)
- 기업 보안 정책

시험 포인트:
- **최고 수준 암호화 + 규정 준수** = DSSE-KMS
- **이중 암호화 = 두 개의 독립적인 AES-256 계층**

### 6. Glue Job Bookmarks - 파일 재처리 문제

**문제 시나리오**: Glue Job Bookmarks 활성화했지만 이전 처리된 S3 파일들이 계속 재처리됨

**해결책: Job Commit 구문 누락**

**Job Bookmarks 작동 원리:**
1. **파일 처리 시작**: Bookmark가 현재 상태 기록
2. **ETL 작업 수행**: 데이터 읽기, 변환, 쓰기
3. **Job Commit**: 성공적 완료 시 Bookmark 상태 업데이트
4. **다음 실행**: 마지막 Commit된 지점부터 시작

**Commit 없을 때 발생하는 문제:**
```python
# 잘못된 예 - Commit 없음
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# S3에서 데이터 읽기
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="my_db",
    table_name="s3_table",
    transformation_ctx="datasource"
)

# Redshift에 데이터 쓰기
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=datasource,
    catalog_connection="redshift-connection",
    connection_options={"dbtable": "target_table"},
    transformation_ctx="redshift_write"
)

# ❌ job.commit() 누락 - Bookmark 업데이트 안됨
```

**올바른 예 - Commit 포함:**
```python
# 올바른 예 - Commit 포함
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# S3에서 데이터 읽기
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="my_db",
    table_name="s3_table",
    transformation_ctx="datasource"
)

# Redshift에 데이터 쓰기
glueContext.write_dynamic_frame.from_jdbc_conf(
    frame=datasource,
    catalog_connection="redshift-connection",
    connection_options={"dbtable": "target_table"},
    transformation_ctx="redshift_write"
)

# ✅ 중요: Job Commit으로 Bookmark 상태 업데이트
job.commit()
```

**Job Commit의 역할:**
- **상태 저장**: 처리 완료된 파일들의 메타데이터 저장
- **Bookmark 업데이트**: DynamoDB에 진행 상황 기록
- **다음 실행 시 참조**: 마지막 Commit 지점부터 새 파일만 처리

**Bookmark 저장소 (DynamoDB):**
```
Table: aws-glue-bookmarks
Key: job-name + run-id
Value: {
  "last_processed_files": [
    "s3://bucket/file1.parquet",
    "s3://bucket/file2.parquet"
  ],
  "last_modified_timestamps": [...],
  "transformation_context": "datasource"
}
```

**Commit 없을 때의 동작:**
```
1차 실행: file1.parquet 처리 → Redshift 저장 → Commit 안함
2차 실행: Bookmark 상태 그대로 → file1.parquet 다시 처리 + file2.parquet 처리
3차 실행: 계속 반복...
```

**Commit 있을 때의 동작:**
```
1차 실행: file1.parquet 처리 → Redshift 저장 → Commit → Bookmark 업데이트
2차 실행: Bookmark 확인 → file2.parquet만 처리 → Commit
3차 실행: file3.parquet만 처리...
```

**디버깅 방법:**
1. **CloudWatch Logs 확인**: Commit 관련 로그 메시지
2. **DynamoDB 테이블 확인**: `aws-glue-bookmarks` 테이블 업데이트 여부
3. **Job 메트릭 확인**: 처리된 레코드 수 변화

**다른 가능한 원인들 (하지만 이 경우는 아님):**
- **Bookmark 비활성화**: 설정에서 확인됨
- **파일 수정 시간 변경**: 동시성 1로 이 문제 배제
- **Transformation Context 문제**: 코드 구조상 문제없음

시험 포인트:
- **Bookmark 활성화 + 파일 재처리** = job.commit() 누락
- **Job Commit = Bookmark 상태 영구 저장**

### 7. MySQL → Redshift 실시간 데이터 마이그레이션

**문제 시나리오**: 온프레미스 MySQL의 PLM 데이터를 실시간으로 Redshift에 복제, 최소 개발 노력

**해결책: AWS DMS (Full Load + CDC)**

**왜 1번 (Glue ETL)이 부적절한가:**

**Glue ETL의 한계:**
- **배치 처리**: 스케줄 기반으로만 실행 (실시간 아님)
- **증분 처리 복잡성**: 변경된 데이터만 식별하려면 복잡한 로직 필요
- **삭제 감지 불가**: DELETE된 레코드 추적 어려움
- **높은 개발 노력**: CDC 로직을 직접 구현해야 함

**Glue ETL 방식의 문제:**
```python
# Glue에서 증분 처리 구현의 복잡성
import sys
from datetime import datetime, timedelta

# 마지막 실행 시점 추적 필요
last_run_time = get_last_bookmark()  # 직접 구현 필요

# 변경된 데이터만 추출 (복잡한 로직)
query = f"""
SELECT * FROM plm_table 
WHERE last_modified > '{last_run_time}'
   OR created_at > '{last_run_time}'
"""

# DELETE된 레코드는 감지 불가
# 복잡한 Upsert 로직 필요
# 트랜잭션 일관성 보장 어려움
```

**DMS의 장점 (2번이 정답인 이유):**

**A. 실시간 CDC (Change Data Capture):**
- **자동 변경 감지**: INSERT, UPDATE, DELETE 모든 변경 실시간 감지
- **트랜잭션 로그 기반**: MySQL binlog를 읽어 실시간 복제
- **순서 보장**: 트랜잭션 순서대로 변경사항 적용

**B. 완전 관리형:**
- **제로 코딩**: SQL 쿼리나 변환 로직 불필요
- **자동 스키마 변환**: MySQL → Redshift 타입 매핑
- **에러 처리**: 자동 재시도 및 DLQ

**C. Full Load + CDC:**
```
Phase 1: Full Load (초기 데이터)
MySQL 전체 데이터 → DMS → Redshift

Phase 2: CDC (실시간 변경)
MySQL binlog → DMS → 실시간 변경사항 → Redshift
```

**DMS 아키텍처:**
```
MySQL (On-premises) → Direct Connect → DMS Replication Instance → Redshift
     ↑ binlog 읽기              ↑ 변환 및 전송              ↑ 실시간 적용
```

**DMS 설정:**
```json
{
  "ReplicationTaskSettings": {
    "TargetMetadata": {
      "TargetSchema": "public",
      "SupportLobs": true,
      "FullLobMode": false
    },
    "FullLoadSettings": {
      "TargetTablePrepMode": "TRUNCATE_BEFORE_LOAD"
    },
    "ChangeProcessingTuning": {
      "BatchApplyEnabled": true,
      "BatchApplyPreserveTransaction": true
    }
  },
  "TableMappings": {
    "rules": [
      {
        "rule-type": "selection",
        "rule-id": "1",
        "rule-name": "plm-tables",
        "object-locator": {
          "schema-name": "plm_db",
          "table-name": "%"
        },
        "rule-action": "include"
      }
    ]
  }
}
```

**실시간성 비교:**

**Glue ETL (배치):**
```
데이터 변경 → 스케줄 대기 (15분~1시간) → ETL 실행 → Redshift 로드
지연시간: 15분 ~ 수시간
```

**DMS CDC (실시간):**
```
데이터 변경 → binlog 감지 (즉시) → DMS 복제 → Redshift 적용
지연시간: 초 ~ 분 단위
```

**개발 노력 비교:**

**Glue ETL 구현 필요사항:**
1. 증분 데이터 식별 로직
2. 북마크 관리 시스템
3. Upsert 로직 구현
4. 에러 처리 및 재시도
5. 스키마 변경 대응
6. 모니터링 및 알람

**DMS 구현 필요사항:**
1. 소스/타겟 엔드포인트 설정
2. 복제 인스턴스 생성
3. 테이블 매핑 설정
4. 작업 시작

**비용 고려사항:**
- **DMS**: 복제 인스턴스 시간당 비용
- **Glue ETL**: 실행 시간당 비용 + 개발/유지보수 비용

**고려사항:**
- **네트워크**: Direct Connect로 안정적인 연결 확보
- **Redshift 동시성**: DMS가 지속적으로 데이터 로드
- **모니터링**: CloudWatch로 복제 지연 모니터링

시험 포인트:
- **실시간 CDC + 최소 개발 노력** = AWS DMS
- **배치 ETL ≠ 실시간 요구사항**
- **DMS = 완전 관리형 데이터베이스 복제**

### 8. Cross-Region, Cross-Account DMS 복제

**문제 시나리오**: Account_A의 eu-east-1 RDS PostgreSQL → Account_B의 eu-west-1 Redshift 마이그레이션

**해결책: 타겟 Account와 Region에 DMS 복제 인스턴스 설정**

**DMS 복제 원리:**
- **복제 인스턴스**: 소스와 타겟 간 데이터 변환 및 전송 담당
- **네트워크 접근**: 소스 및 타겟 모두 접근 가능한 위치 필요
- **권한**: 양쪽 데이터스토어 접근 권한 필요

**왜 Account_B의 eu-west-1인가:**

**A. 네트워크 최적화:**
- **타겟 근처 배치**: Redshift와 동일 리전으로 전송 지연 최소화
- **대역폭 효율성**: 대량 데이터 로드 시 네트워크 비용 최소화
- **동일 AZ**: 가능하면 Redshift와 같은 AZ에 배치

**B. 계정 권한 관리:**
- **타겟 계정**: Account_B에서 Redshift 접근 권한 직접 관리
- **Cross-Account 역할**: Account_A RDS 접근용 역할 생성
- **단순한 권한 체인**: 복잡한 권한 위임 최소화

**아키텍처:**
```
Account_A (eu-east-1)           Account_B (eu-west-1)
┌─────────────────┐            ┌─────────────────┐
│ RDS PostgreSQL  │────────────│ DMS 복제인스턴스 │
│                 │ Cross-Account│                │
└─────────────────┘    Role    │ ↓               │
                               │ Redshift        │
                               └─────────────────┘
```

**설정 단계:**

**1. Account_A에서 Cross-Account 역할 생성:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::ACCOUNT_B:root"
      },
      "Action": "sts:AssumeRole",
      "Condition": {
        "StringEquals": {
          "sts:ExternalId": "dms-migration-key"
        }
      }
    }
  ]
}
```

**2. RDS 접근 권한 정책:**
```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "rds:DescribeDBInstances",
        "rds:DescribeDBClusters"
      ],
      "Resource": "*"
    }
  ]
}
```

**3. Account_B에서 DMS 설정:**
```bash
# DMS 복제 인스턴스 생성 (eu-west-1)
aws dms create-replication-instance \
  --replication-instance-identifier cross-account-replication \
  --replication-instance-class dms.t3.medium \
  --vpc-security-group-ids sg-12345678 \
  --availability-zone eu-west-1a
```

**4. 소스 엔드포인트 설정 (Cross-Account):**
```json
{
  "EndpointIdentifier": "source-postgres-cross-account",
  "EndpointType": "source",
  "EngineName": "postgres",
  "ServerName": "rds-instance.eu-east-1.rds.amazonaws.com",
  "Port": 5432,
  "DatabaseName": "production",
  "Username": "dms_user",
  "Password": "secure_password",
  "ExtraConnectionAttributes": "region=eu-east-1;roleArn=arn:aws:iam::ACCOUNT_A:role/DMSCrossAccountRole"
}
```

**5. 타겟 엔드포인트 설정 (Local):**
```json
{
  "EndpointIdentifier": "target-redshift-local",
  "EndpointType": "target",
  "EngineName": "redshift",
  "ServerName": "redshift-cluster.eu-west-1.redshift.amazonaws.com",
  "Port": 5439,
  "DatabaseName": "warehouse",
  "Username": "dms_user",
  "Password": "secure_password"
}
```

**네트워크 고려사항:**

**VPC 피어링 또는 Transit Gateway:**
```
Account_A VPC (eu-east-1) ←→ Account_B VPC (eu-west-1)
        ↑                              ↑
    RDS PostgreSQL              DMS + Redshift
```

**보안 그룹 설정:**
```bash
# Account_A RDS 보안 그룹
aws ec2 authorize-security-group-ingress \
  --group-id sg-rds-source \
  --protocol tcp \
  --port 5432 \
  --source-group sg-dms-replication

# Account_B DMS 보안 그룹  
aws ec2 authorize-security-group-egress \
  --group-id sg-dms-replication \
  --protocol tcp \
  --port 5432 \
  --destination-group sg-rds-source
```

**대안 위치들과 비교:**

**Account_A eu-east-1 (소스 근처):**
- ❌ Redshift 접근 복잡
- ❌ 대량 데이터 Cross-Region 전송 비용

**Account_B eu-east-1 (소스와 동일):**
- ❌ Redshift Cross-Region 접근
- ❌ 타겟과 멀어 성능 저하

**Account_A eu-west-1 (타겟과 동일):**
- ❌ Cross-Account + Cross-Region 복잡성
- ❌ 소스 접근 권한 복잡

시험 포인트:
- **Cross-Account 마이그레이션** = 타겟 계정에 DMS 배치
- **성능 최적화** = 타겟과 동일 리전에 배치
- **권한 단순화** = 타겟 계정에서 직접 관리

### 9. Redshift COPY 명령 최적화 - Manifest 파일 사용

**문제 시나리오**: 여러 개별 COPY 명령으로 인한 느린 데이터 로딩, 비용 증가 없이 성능 향상 필요

**해결책: Manifest 파일을 이용한 단일 COPY 명령**

**현재 방식의 문제점:**

**개별 COPY 명령 방식:**
```sql
-- 현재 방식 (비효율적)
COPY target_table FROM 's3://data-lake/source1/file1.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
DELIMITER ',';

COPY target_table FROM 's3://data-lake/source1/file2.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
DELIMITER ',';

COPY target_table FROM 's3://data-lake/source2/file3.csv'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
DELIMITER ',';

-- ... 수백 개의 COPY 명령
```

**성능 문제:**
- **순차 처리**: 각 COPY 명령이 순서대로 실행
- **연결 오버헤드**: 각 명령마다 새로운 연결 설정
- **메타데이터 처리**: 매번 테이블 메타데이터 확인
- **트랜잭션 오버헤드**: 각 COPY마다 별도 트랜잭션

**Manifest 파일 방식의 장점:**

**A. 병렬 처리:**
- **동시 로딩**: 여러 파일을 병렬로 로드
- **슬라이스 분산**: Redshift 노드 간 작업 분산
- **네트워크 최적화**: 연결 재사용

**B. 단일 트랜잭션:**
- **원자성**: 모든 파일이 성공하거나 모두 실패
- **일관성**: 부분 로드 상태 방지
- **메타데이터 효율성**: 한 번의 테이블 락

**Manifest 파일 생성:**
```json
{
  "entries": [
    {
      "url": "s3://data-lake/source1/file1.csv",
      "mandatory": true
    },
    {
      "url": "s3://data-lake/source1/file2.csv", 
      "mandatory": true
    },
    {
      "url": "s3://data-lake/source2/file3.csv",
      "mandatory": true
    },
    {
      "url": "s3://data-lake/source2/file4.csv",
      "mandatory": true
    },
    {
      "url": "s3://data-lake/source3/",
      "mandatory": true
    }
  ]
}
```

**최적화된 COPY 명령:**
```sql
-- 최적화된 방식 (효율적)
COPY target_table 
FROM 's3://data-lake/manifest.json'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
MANIFEST
DELIMITER ','
GZIP
ACCEPTINVCHARS
COMPUPDATE OFF
STATUPDATE OFF;
```

**성능 향상 요인:**

**1. 병렬성:**
```
기존: File1 → File2 → File3 → File4 (순차)
개선: File1 + File2 + File3 + File4 (병렬)
```

**2. 네트워크 최적화:**
```
기존: 4개 연결 × 각각 설정 오버헤드
개선: 1개 연결로 모든 파일 처리
```

**3. 메타데이터 효율성:**
```
기존: 4번의 테이블 메타데이터 접근
개선: 1번의 테이블 메타데이터 접근
```

**Manifest 파일 자동 생성:**
```python
import boto3
import json

def create_manifest_file(bucket, prefix, manifest_path):
    s3 = boto3.client('s3')
    
    # S3에서 파일 목록 가져오기
    response = s3.list_objects_v2(
        Bucket=bucket,
        Prefix=prefix
    )
    
    # Manifest 구조 생성
    entries = []
    for obj in response.get('Contents', []):
        if obj['Key'].endswith('.csv'):
            entries.append({
                "url": f"s3://{bucket}/{obj['Key']}",
                "mandatory": True
            })
    
    manifest = {"entries": entries}
    
    # Manifest 파일 S3에 업로드
    s3.put_object(
        Bucket=bucket,
        Key=manifest_path,
        Body=json.dumps(manifest, indent=2),
        ContentType='application/json'
    )
    
    return f"s3://{bucket}/{manifest_path}"

# 사용 예시
manifest_url = create_manifest_file(
    bucket='data-lake',
    prefix='daily-data/',
    manifest_path='manifests/daily-manifest.json'
)
```

**추가 최적화 옵션:**

**압축 활용:**
```sql
COPY target_table 
FROM 's3://data-lake/manifest.json'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
MANIFEST
GZIP  -- 압축된 파일 처리
DELIMITER ','
```

**병렬 처리 최대화:**
```sql
COPY target_table 
FROM 's3://data-lake/manifest.json'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole'
MANIFEST
DELIMITER ','
ACCEPTINVCHARS  -- 잘못된 문자 허용으로 중단 방지
MAXERROR 1000   -- 오류 허용 한도
```

**성능 비교:**

**개별 COPY (기존):**
- 100개 파일: ~30분
- CPU 사용률: 낮음 (순차 처리)
- I/O 대기: 높음

**Manifest COPY (개선):**
- 100개 파일: ~5분 (6배 향상)
- CPU 사용률: 높음 (병렬 처리)
- I/O 최적화: 네트워크 대역폭 최대 활용

**비용 영향:**
- **컴퓨팅 비용**: 변화 없음 (동일한 데이터 처리)
- **네트워크 비용**: 약간 감소 (효율적 전송)
- **시간 비용**: 대폭 절약 (빠른 로딩)

시험 포인트:
- **여러 파일 → 단일 테이블 최적화** = Manifest 파일
- **병렬 처리 + 성능 향상** = 여러 개별 COPY → 단일 Manifest COPY
- **비용 증가 없는 성능 개선** = Manifest 사용


### 10. Kinesis Data Firehose - CSV → JSON → Parquet 변환

**문제 시나리오**: 2MB CSV 파일을 JSON으로 변환 후 Parquet 형식으로 S3 저장, 최소 개발 노력

**해결책: Lambda 변환 + Firehose 네이티브 Parquet 변환**

**데이터 변환 파이프라인:**
```
CSV 입력 → Lambda (CSV→JSON) → Firehose (JSON→Parquet) → S3 저장
```

**왜 Lambda가 CSV→JSON 변환에 필요한가:**

**Firehose 네이티브 변환 한계:**
- **JSON → Parquet**: 네이티브 지원 ✅
- **CSV → JSON**: 네이티브 지원 없음 ❌
- **CSV → Parquet**: 직접 변환 불가 ❌

**Lambda 변환 함수 (CSV → JSON):**
```python
import json
import csv
import base64
from io import StringIO

def lambda_handler(event, context):
    output = []
    
    for record in event['records']:
        # Base64 디코딩
        payload = base64.b64decode(record['data']).decode('utf-8')
        
        # CSV 파싱
        csv_reader = csv.DictReader(StringIO(payload))
        
        json_records = []
        for row in csv_reader:
            json_records.append(row)
        
        # JSON 변환
        json_output = '\n'.join([json.dumps(record) for record in json_records])
        
        # Base64 인코딩하여 반환
        output_record = {
            'recordId': record['recordId'],
            'result': 'Ok',
            'data': base64.b64encode(json_output.encode('utf-8')).decode('utf-8')
        }
        output.append(output_record)
    
    return {'records': output}
```

**Firehose 설정 (JSON → Parquet):**
```json
{
  "DeliveryStreamName": "csv-to-parquet-stream",
  "ExtendedS3DestinationConfiguration": {
    "RoleARN": "arn:aws:iam::account:role/firehose-delivery-role",
    "BucketARN": "arn:aws:s3:::data-lake-bucket",
    "Prefix": "parquet-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/",
    "BufferingHints": {
      "SizeInMBs": 128,
      "IntervalInSeconds": 60
    },
    "CompressionFormat": "UNCOMPRESSED",
    "DataFormatConversionConfiguration": {
      "Enabled": true,
      "OutputFormatConfiguration": {
        "Serializer": {
          "ParquetSerDe": {}
        }
      },
      "SchemaConfiguration": {
        "DatabaseName": "data_catalog",
        "TableName": "converted_data",
        "Region": "us-west-2",
        "VersionId": "LATEST"
      }
    },
    "ProcessingConfiguration": {
      "Enabled": true,
      "Processors": [
        {
          "Type": "Lambda",
          "Parameters": [
            {
              "ParameterName": "LambdaArn",
              "ParameterValue": "arn:aws:lambda:us-west-2:account:function:csv-to-json-transformer"
            }
          ]
        }
      ]
    }
  }
}
```

**Parquet 변환이 Lambda 없이 가능한 이유:**

**Firehose 네이티브 Parquet 변환:**
- **내장 기능**: JSON → Parquet 자동 변환
- **스키마 추론**: Glue Data Catalog 기반 스키마 적용
- **열 압축**: 자동 columnar 저장 최적화
- **파티셔닝**: 자동 파티션 생성

**Data Format Conversion 설정:**
```json
{
  "DataFormatConversionConfiguration": {
    "Enabled": true,
    "OutputFormatConfiguration": {
      "Serializer": {
        "ParquetSerDe": {
          "BlockSizeBytes": 256000000,
          "PageSizeBytes": 1000000,
          "Compression": "SNAPPY",
          "EnableDictionaryCompression": false,
          "MaxPaddingBytes": 0
        }
      }
    }
  }
}
```

**전체 아키텍처:**
```
데이터 소스 (CSV) 
    ↓
Kinesis Data Firehose
    ↓
Lambda Function (CSV → JSON 변환)
    ↓
Firehose 네이티브 변환 (JSON → Parquet)
    ↓
S3 (Parquet 파일)
```

**개발 노력 비교:**

**제안된 방식 (최소 노력):**
1. Lambda 함수 1개 (CSV → JSON)
2. Firehose 설정 (네이티브 Parquet 변환)
3. Glue Catalog 스키마 정의

**대안 방식들 (더 많은 노력):**

**완전 Lambda 방식:**
```python
# 더 복잡한 Lambda (CSV → Parquet)
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq

def lambda_handler(event, context):
    # CSV 읽기
    df = pd.read_csv(StringIO(csv_data))
    
    # Parquet 변환 (직접 구현)
    table = pa.Table.from_pandas(df)
    pq.write_table(table, output_path)
    
    # S3 업로드 로직...
```

**Glue ETL 방식:**
```python
# Glue Job으로 배치 처리
datasource = glueContext.create_dynamic_frame.from_catalog(
    database="csv_db",
    table_name="csv_table"
)

# 복잡한 변환 로직...
parquet_data = datasource.toDF().write.parquet(s3_path)
```

**성능 고려사항:**

**Lambda 제한:**
- **메모리**: 최대 10GB (2MB 파일에 충분)
- **실행 시간**: 최대 15분
- **동시 실행**: 1000개 (확장 가능)

**Firehose 버퍼링:**
- **배치 크기**: 128MB 또는 60초
- **압축**: Snappy 자동 압축
- **파티셔닝**: 시간 기반 자동 파티션

**비용 분석:**
- **Lambda**: 실행 시간 × 메모리 사용량
- **Firehose**: 처리된 데이터량
- **S3**: 저장 비용 (Parquet 압축으로 절약)

시험 포인트:
- **CSV → JSON 변환** = Lambda 필요
- **JSON → Parquet 변환** = Firehose 네이티브 지원
- **최소 개발 노력** = Lambda(포맷 변환) + Firehose(압축/저장)


### 11. AWS Transfer Family - TLS 1.2 보안 정책 설정

**문제 시나리오**: 온프레미스 → AWS 데이터 마이그레이션 시 TLS 1.2 이상 암호화 필수

**해결책: Transfer Family 서버 보안 정책 업데이트**

**AWS Transfer Family 보안 정책 개념:**
- **TLS 프로토콜 버전**: 클라이언트-서버 간 암호화 수준 제어
- **암호화 스위트**: 허용되는 암호화 알고리즘 정의
- **규정 준수**: 기업 보안 정책 및 규제 요구사항 충족

**보안 정책 설정 방법:**

**A. AWS CLI 사용:**
```bash
# 기존 Transfer Family 서버 보안 정책 업데이트
aws transfer update-server \
  --server-id s-1234567890abcdef0 \
  --protocols SFTP \
  --security-policy-name "TransferSecurityPolicy-TLS-1-2-2019-07"
```

**B. CloudFormation 템플릿:**
```yaml
AWSTemplateFormatVersion: '2010-09-09'
Resources:
  TransferServer:
    Type: AWS::Transfer::Server
    Properties:
      Protocols:
        - SFTP
      SecurityPolicyName: "TransferSecurityPolicy-TLS-1-2-2019-07"
      EndpointType: PUBLIC
      IdentityProviderType: SERVICE_MANAGED
      LoggingRole: !GetAtt TransferLoggingRole.Arn
      Tags:
        - Key: Name
          Value: SecureTransferServer
```

**C. Terraform 설정:**
```hcl
resource "aws_transfer_server" "secure_server" {
  protocols             = ["SFTP"]
  security_policy_name  = "TransferSecurityPolicy-TLS-1-2-2019-07"
  endpoint_type        = "PUBLIC"
  identity_provider_type = "SERVICE_MANAGED"
  
  tags = {
    Name = "SecureTransferServer"
  }
}
```

**사용 가능한 보안 정책들:**

**1. TransferSecurityPolicy-TLS-1-2-2019-07 (권장):**
- **TLS 버전**: 1.2 이상만 허용
- **암호화 스위트**: 강력한 암호화 알고리즘만 지원
- **규정 준수**: 대부분의 기업 보안 정책 충족

**2. TransferSecurityPolicy-TLS-1-1-2017-01 (레거시):**
- **TLS 버전**: 1.1, 1.2 허용
- **보안 수준**: 상대적으로 낮음
- **사용 권장**: 호환성 문제 시에만

**3. TransferSecurityPolicy-TLS-1-0-2017-01 (비권장):**
- **TLS 버전**: 1.0, 1.1, 1.2 허용
- **보안 위험**: TLS 1.0은 취약점 존재
- **사용 금지**: 보안 정책 위반

**보안 정책별 지원 암호화 스위트:**

**TLS-1-2-2019-07 스위트:**
```
TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305
TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384
```

**클라이언트 호환성 확인:**
```bash
# SFTP 클라이언트에서 TLS 버전 확인
sftp -v -o "Ciphers=aes128-gcm@openssh.com,aes256-gcm@openssh.com" user@transfer-server
```

**보안 정책 검증:**
```bash
# 서버 설정 확인
aws transfer describe-server --server-id s-1234567890abcdef0

# 출력 예시
{
  "Server": {
    "Arn": "arn:aws:transfer:us-east-1:123456789012:server/s-1234567890abcdef0",
    "SecurityPolicyName": "TransferSecurityPolicy-TLS-1-2-2019-07",
    "Protocols": ["SFTP"],
    "State": "ONLINE"
  }
}
```

**마이그레이션 시 고려사항:**

**1. 클라이언트 업데이트:**
```bash
# 온프레미스 SFTP 클라이언트 TLS 1.2 지원 확인
openssl s_client -connect transfer-server:22 -tls1_2
```

**2. 네트워크 보안:**
```bash
# VPC 엔드포인트 사용 시 추가 보안
aws transfer create-server \
  --endpoint-type VPC \
  --endpoint-details VpcEndpointId=vpce-12345678,VpcId=vpc-12345678
```

**3. 로깅 및 모니터링:**
```json
{
  "LoggingRole": "arn:aws:iam::account:role/TransferLoggingRole",
  "CloudWatchLogGroup": "/aws/transfer/server-logs"
}
```

**규정 준수 관점:**

**PCI DSS 요구사항:**
- TLS 1.2 이상 필수
- 강력한 암호화 알고리즘 요구

**HIPAA 준수:**
- 전송 중 데이터 암호화 필수
- 보안 정책 문서화 요구

**SOC 2 Type II:**
- 보안 구성 관리
- 정기적인 보안 정책 검토

**문제 해결:**
```bash
# 연결 실패 시 디버깅
aws logs filter-log-events \
  --log-group-name /aws/transfer/s-1234567890abcdef0 \
  --filter-pattern "TLS"
```

시험 포인트:
- **TLS 1.2 이상 강제** = Transfer Family 보안 정책 업데이트
- **규정 준수 + 데이터 전송 보안** = TransferSecurityPolicy-TLS-1-2-2019-07
- **기업 보안 정책 충족** = 적절한 보안 정책 선택

