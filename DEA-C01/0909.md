## 0909 (page ~8)

### 1. 실시간 센서 데이터 모니터링 - 최저 지연 시간 솔루션

**문제 시나리오**: 제조공장 센서 데이터를 Kinesis Data Streams로 수집 후 Firehose를 통해 S3 저장, 대형 스크린에 실시간 운영 효율성 표시, 최저 지연 시간 요구

**정답: A. Amazon Managed Service for Apache Flink → Timestream → Grafana 대시보드**

**왜 Flink + Timestream + Grafana가 최적인가:**

**A. 실시간 스트림 처리 아키텍처:**
```
Kinesis Data Stream → Managed Flink → Timestream → Grafana Dashboard
        ↑                   ↑            ↑           ↑
    센서 데이터 수집      실시간 처리    시계열 저장    즉시 시각화
    (밀리초 지연)       (밀리초 지연)   (마이크로초)   (1초 리프레시)
```

**B. Flink의 저지연 처리 장점:**

**스트림 처리 방식:**
- **연속 처리**: 데이터 도착 즉시 처리 (배치 대기 없음)
- **인메모리 상태**: 중간 집계 결과를 메모리에 유지
- **병렬 처리**: 다중 파티션 동시 처리로 처리량 극대화
- **윈도우 연산**: 실시간 집계 (1분, 5분 간격 메트릭)

**실시간 집계 예시:**
- 생산 라인별 처리량 (건/분)
- 장비 가동률 (실시간 %)
- 품질 지표 (실시간 불량률)
- 에너지 효율성 (kWh/제품)

**C. Timestream의 시계열 최적화:**

**고성능 쓰기/읽기:**
- **쓰기 성능**: 초당 수백만 포인트 인제스트
- **자동 파티셔닝**: 시간 기반 자동 분산 저장
- **인메모리 캐싱**: 최근 데이터 빠른 조회
- **압축 저장**: 장기 데이터 비용 효율적 저장

**시계열 쿼리 최적화:**
```sql
-- 실시간 대시보드용 쿼리 (밀리초 응답)
SELECT time, line_id, AVG(throughput) as avg_throughput
FROM sensor_metrics 
WHERE time >= ago(1h)
GROUP BY time(1m), line_id
```

**D. Grafana vs QuickSight 지연 시간 비교:**

**Grafana (정답 A):**
- ✅ **실시간 리프레시**: 1초 간격 자동 업데이트
- ✅ **직접 연결**: Timestream 네이티브 커넥터
- ✅ **스트리밍 모드**: 데이터 도착 즉시 시각화
- ✅ **대형 스크린 최적화**: 실시간 전용 대시보드

**QuickSight (오답 C):**
- ❌ **배치 리프레시**: 최소 1분 간격 업데이트
- ❌ **캐싱 지연**: 데이터 캐시로 인한 추가 지연
- ❌ **BI 도구 특성**: 분석용으로 실시간 모니터링에 부적합

**E. 전체 지연 시간 분석:**

**옵션 A (정답) - 총 지연시간 < 5초:**
```
센서 → Kinesis (1초) → Flink (1초) → Timestream (1초) → Grafana (1초) = 4초
```

**옵션 C (오답) - 총 지연시간 > 60초:**
```
센서 → Kinesis (1초) → Flink (1초) → Firehose (60초) → Timestream (1초) → QuickSight (60초) = 123초
```

**F. Firehose vs Direct Write 차이:**

**Firehose 사용 시 (오답):**
- ❌ **배치 지연**: 최소 60초 버퍼링
- ❌ **대용량 최적화**: 실시간보다 처리량 중심
- ❌ **배치 쓰기**: 대량 인서트 방식

**Direct Connector 사용 시 (정답):**
- ✅ **즉시 쓰기**: 레코드별 즉시 전송
- ✅ **실시간 최적화**: 지연 시간 최소화
- ✅ **스트림 쓰기**: 연속 데이터 플로우

시험 포인트:
- **실시간 모니터링 + 최저 지연** = Flink + Timestream + Grafana
- **대형 스크린 실시간 표시** = 배치 처리 방식 배제
- **센서 데이터 실시간 처리** = 스트림 처리 > 배치 처리

---

### 2. Amazon Redshift Concurrency Scaling - 프로비저닝 vs 서버리스

**문제 시나리오**: RA3 노드 Redshift 클러스터, 읽기/쓰기 용량 확장 필요, Concurrency Scaling 활성화 방법

**정답: B. WLM 큐 레벨에서 Concurrency Scaling 활성화**

**왜 B가 정답이고 A가 틀린가:**

**A. Redshift 아키텍처 차이점 이해:**

**Redshift Provisioned Cluster (문제 상황):**
- RA3 노드로 구성된 **기존 프로비저닝된 클러스터**
- 고정된 컴퓨팅 리소스 (노드 수 고정)
- WLM 큐를 통한 워크로드 관리
- Concurrency Scaling을 통한 **추가 클러스터** 자동 생성

**Redshift Serverless (문제와 무관):**
- **완전히 다른 서비스** (클러스터가 아님)
- 워크그룹 기반 관리
- 자동 확장/축소 (별도 설정 불필요)

**B. 정답 B의 핵심 - WLM 큐 레벨 설정:**

**WLM Configuration에서 Concurrency Scaling 활성화:**
```json
{
  "wlm_configuration": [
    {
      "query_group": "dashboard",
      "memory_percent_to_use": 30,
      "max_concurrency_scaling_clusters": 10,
      "concurrency_scaling_mode": "auto"
    }
  ]
}
```

**큐별 세부 설정:**
- **max_concurrency_scaling_clusters**: 큐당 최대 확장 클러스터 수
- **concurrency_scaling_mode**: auto (자동 확장)
- **target_queue_time**: 큐 대기시간 임계값 (기본 5분)

**C. 오답 A가 틀린 이유:**

**Redshift Serverless는 다른 서비스:**
- RA3 노드 클러스터 ≠ Redshift Serverless
- 기존 클러스터를 서버리스로 전환할 수 없음
- 서버리스는 자체적으로 auto-scaling (별도 concurrency scaling 불필요)

**아키텍처적 차이:**
```
기존 클러스터 (문제 상황):
RA3 Cluster → WLM Queues → Concurrency Scaling Clusters
    ↑            ↑              ↑
고정 리소스    큐별 관리      추가 클러스터 자동 생성

서버리스 (문제와 무관):
Workgroup → Auto Scaling
    ↑           ↑
가변 리소스   자동 확장/축소
```

**D. Concurrency Scaling 작동 원리:**

**자동 확장 조건:**
1. **큐 대기시간** > 5분 (기본값)
2. **큐의 동시 쿼리 수** > 설정된 슬롯 수
3. **클러스터 CPU/메모리** > 임계값

**확장 프로세스:**
```
대기 큐 발생 → 임계값 초과 → 추가 클러스터 생성 → 쿼리 분산 실행
     ↑            ↑              ↑              ↑
높은 동시성    5분 대기 감지    자동 프로비저닝   성능 향상
```

**E. 비용 및 성능 고려사항:**

**Concurrency Scaling 비용:**
- **무료 크레딧**: 24시간/월 제공
- **초과 사용**: 온디맨드 요금 (시간당)
- **비용 최적화**: 큐별 max_clusters 제한

**성능 효과:**
- **읽기 쿼리**: 최대 10배 동시 처리 향상
- **쓰기 쿼리**: 메인 클러스터에서만 실행 (확장 불가)
- **혼합 워크로드**: 읽기 성능 크게 향상

**F. 실무 설정 예시:**

**대시보드 전용 큐 (읽기 중심):**
```json
{
  "query_group": "dashboard",
  "memory_percent_to_use": 20,
  "max_concurrency_scaling_clusters": 5
}
```

**ETL 전용 큐 (쓰기 중심):**
```json
{
  "query_group": "etl",
  "memory_percent_to_use": 60,
  "max_concurrency_scaling_clusters": 0
}
```

시험 포인트:
- **RA3 클러스터 확장** = WLM 큐 레벨 Concurrency Scaling
- **프로비저닝드 vs 서버리스** = 완전히 다른 서비스 구분
- **읽기 성능 향상** = Concurrency Scaling 주요 목적

---

### 3. 대규모 온프레미스 워크로드 AWS 마이그레이션 - EMR 선택 이유

**문제 시나리오**: Apache Pig, Oozie, Spark, HBase, Flink 사용하는 온프레미스 워크로드, 페타바이트급 데이터를 초단위 처리, 서버리스 옵션 탐색하면서 운영 오버헤드 최소화, 동등 이상 성능 유지 

**정답: Amazon EMR (Elastic MapReduce)**

**왜 EMR이 최적 선택인가:**

**A. 기존 워크로드 완벽 호환성:**

**지원하는 프레임워크:**
- ✅ **Apache Spark**: EMR의 핵심 엔진, 네이티브 지원
- ✅ **Apache HBase**: NoSQL 데이터베이스, EMR 클러스터에서 실행
- ✅ **Apache Flink**: 스트림 처리, EMR 6.x에서 지원
- ✅ **Apache Pig**: 데이터 플로우 언어, EMR 기본 포함
- ✅ **Apache Oozie**: 워크플로우 스케줄러, EMR에서 실행 가능

**기존 코드 재사용:**
- 기존 Spark 애플리케이션 그대로 실행
- HBase 스키마 및 데이터 마이그레이션 지원
- Pig Latin 스크립트 호환성 유지

**B. 페타바이트급 고성능 처리 능력:**

**확장성 아키텍처:**
```
EMR Master Node → Core Nodes (HDFS + 컴퓨팅) + Task Nodes (컴퓨팅 전용)
     ↑                ↑                           ↑
워크플로우 관리      데이터 저장 + 처리            추가 처리 용량
```

**성능 최적화 기능:**
- **EMR Runtime for Spark**: 3.2배 빠른 성능 (표준 Spark 대비)
- **EMRFS**: S3 최적화 파일 시스템, 페타바이트급 데이터 접근
- **Spot Fleet**: 최대 90% 비용 절감으로 대규모 클러스터 운영
- **다양한 인스턴스 타입**: 메모리 최적화(R5), 컴퓨팅 최적화(C5), 스토리지 최적화(I3)

**C. 서버리스 vs EMR 선택 이유:**

**서버리스 옵션들의 한계:**

**AWS Glue:**
- ❌ **DPU 제한**: 최대 100 DPU (약 400 vCPU)
- ❌ **프레임워크 제한**: HBase, Oozie 미지원
- ❌ **성능**: 페타바이트급 초단위 처리에 부적합

**Lambda:**
- ❌ **실행 시간**: 15분 제한
- ❌ **메모리**: 10GB 제한
- ❌ **빅데이터 프레임워크**: Spark, HBase 실행 불가

**EMR Serverless:**
- 🔶 **부분 호환**: Spark, Hive만 지원 (HBase, Flink, Oozie 미지원)
- 🔶 **성능**: 대규모 클러스터만큼의 성능 미보장

**D. EMR의 운영 오버헤드 최소화:**

**관리형 서비스 이점:**
- **자동 프로비저닝**: 클러스터 생성/종료 자동화
- **Auto Scaling**: 워크로드에 따른 자동 확장/축소
- **패치 관리**: EMR 버전 업그레이드 자동화
- **모니터링**: CloudWatch 통합 모니터링

**Step 실행 모드:**
```bash
# 일시적 클러스터 - 작업 완료 후 자동 종료
aws emr create-cluster \
  --steps file://spark-steps.json \
  --auto-terminate \
  --enable-debugging
```

**E. EMR vs 다른 옵션 비교:**

**EMR vs 자체 관리 Spark on EC2:**
- ✅ **운영 오버헤드**: EMR이 대폭 감소
- ✅ **성능**: EMR Runtime 최적화
- ✅ **비용**: Spot Fleet 활용

**EMR vs EKS (Kubernetes):**
- ✅ **빅데이터 최적화**: EMR이 특화됨
- ✅ **프레임워크 지원**: HBase, Oozie 등 완벽 지원
- ✅ **운영 복잡성**: EMR이 단순함

**F. 실제 마이그레이션 아키텍처:**

**페타바이트급 데이터 처리 구성:**
```
S3 (페타바이트 데이터) → EMR Cluster (수백 노드) → 결과 저장
    ↑                      ↑                      ↓
EMRFS 최적화 접근      Spark/HBase/Flink      S3/Redshift/RDS
```

**성능 벤치마크:**
- **데이터 처리**: 1PB 데이터를 1시간 이내 처리 (1000노드 클러스터)
- **스트림 처리**: Flink로 초당 수백만 이벤트 처리
- **배치 처리**: Spark로 복잡한 ETL 작업 병렬 실행

**G. 비용 최적화 전략:**

**Spot Instance 활용:**
- Task 노드: 100% Spot (최대 90% 절약)
- Core 노드: 50% Spot + 50% On-Demand (안정성 확보)
- Master 노드: On-Demand (안정성 보장)

**예상 비용 (1000노드 기준):**
- On-Demand: $10,000/월
- Spot Fleet: $2,000/월 (80% 절약)

시험 포인트:
- **기존 프레임워크 호환성** = EMR 완벽 지원
- **페타바이트급 고성능** = EMR 전용 최적화
- **서버리스 한계** = 대규모 처리에서 성능/호환성 부족
