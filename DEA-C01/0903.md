## 0903 (~page 20)

### 1. Apache Flink vs Kinesis Data Analytics

**Apache Flink (formerly Amazon Kinesis Data Analytics)**

정의: 실시간 스트림 및 배치 데이터 처리를 위한 오픈소스 분산 처리 엔진

핵심 특징:
- Event Time Processing: 이벤트가 실제로 발생한 시간 기준 처리
- Stateful Processing: 애플리케이션 상태 유지 및 장애 복구
- Low Latency: 밀리초 단위 지연 시간
- Exactly-once Semantics: 정확히 한 번 처리 보장

AWS 통합:
- Kinesis Data Analytics for Apache Flink: 완전 관리형 서비스
- Auto Scaling: 처리량에 따른 자동 확장
- Checkpointing: S3 기반 상태 백업

시험 포인트:
- 실시간 복잡한 이벤트 처리 시나리오
- 상태 유지가 필요한 스트리밍 애플리케이션
- 정확한 처리 순서와 중복 방지가 중요한 경우

### 2. CSV to Parquet 변환 (비용 최적화)

문제 시나리오: 15개 컬럼 CSV 파일, 1-2개 컬럼만 쿼리, 전체 파일 스캔 드물음

**해결책: AWS Glue ETL + Parquet 변환**

Parquet 형식의 장점:
- 열 지향 저장: 필요한 컬럼만 스캔
- 압축 효율성: CSV 대비 80-90% 저장 공간 절약
- 스키마 진화: 컬럼 추가/삭제 용이
- 빠른 쿼리 성능: Athena 최적화

비용 효과:
- 저장 비용: 압축으로 S3 비용 절약
- 쿼리 비용: 스캔 데이터량 감소로 Athena 비용 절약
- 성능 향상: 쿼리 실행 시간 단축

시험 포인트:
- 컬럼 선택적 쿼리 + 비용 최적화 = Parquet 변환

### 3. Lake Formation 지역별 데이터 액세스 제어

문제 시나리오: 5개 지역별 HR 부서, 각자 지역 직원 데이터만 접근

**해결책: Lake Formation 위치 등록 + 세분화된 액세스 제어**

**1. S3 경로를 Lake Formation 위치로 등록**
- 데이터 레이크 위치 등록: Lake Formation이 관리할 S3 경로 지정
- 중앙집중식 관리: 여러 지역의 데이터를 하나의 카탈로그로 통합
- 권한 위임: IAM에서 Lake Formation으로 권한 관리 이관

**2. 세분화된 액세스 제어 + 지역별 데이터 필터**
- Row-level Security: 지역 컬럼 기반 행 수준 필터링
- 데이터 필터: WHERE region = 'us-east-1' 형태의 자동 필터
- IAM Role 매핑: 각 HR 부서 역할과 해당 지역 필터 연결

Lake Formation 장점:
- 최소 운영 오버헤드: 중앙집중식 권한 관리
- 자동 필터링: 쿼리 시 자동으로 권한에 맞는 데이터만 반환
- Cross-region Support: 여러 지역 데이터 통합 관리

시험 포인트:
- 지역별/부서별 데이터 격리 + 최소 운영 오버헤드 = Lake Formation

### 4. Step Functions EMR 통합

문제 시나리오: Step Functions에서 EMR 작업 생성 및 실행

**Step Functions EMR 통합 패턴:**

```json
{
  "Comment": "EMR 작업 실행 워크플로우",
  "StartAt": "CreateEMRCluster",
  "States": {
    "CreateEMRCluster": {
      "Type": "Task",
      "Resource": "arn:aws:states:::elasticmapreduce:createCluster.sync",
      "Parameters": {
        "Name": "MyEMRCluster",
        "ReleaseLabel": "emr-6.4.0",
        "Applications": [{"Name": "Spark"}],
        "ServiceRole": "arn:aws:iam::account:role/EMR_DefaultRole",
        "JobFlowRole": "arn:aws:iam::account:role/EMR_EC2_DefaultRole"
      },
      "Next": "AddStepToCluster"
    },
    "AddStepToCluster": {
      "Type": "Task", 
      "Resource": "arn:aws:states:::elasticmapreduce:addStep.sync",
      "Parameters": {
        "ClusterId.$": "$.ClusterId",
        "Step": {
          "Name": "SparkStep",
          "ActionOnFailure": "TERMINATE_CLUSTER",
          "HadoopJarStep": {
            "Jar": "command-runner.jar",
            "Args": ["spark-submit", "s3://bucket/script.py"]
          }
        }
      },
      "End": true
    }
  }
}
```

핵심 IAM 권한:
- elasticmapreduce:CreateCluster
- elasticmapreduce:AddStep
- elasticmapreduce:DescribeCluster
- iam:PassRole (EMR 서비스 역할용)

시험 포인트:
- EMR 클러스터 생명주기 관리
- 동기식 실행 (.sync 접미사)
- 단계별 작업 오케스트레이션

### 5. Athena에서 Spark 사용

문제 시나리오: Athena CTAS(SQL) 대신 Spark를 사용한 분석

**해결책: Athena Workgroup이 아닌 다른 서비스 필요**

올바른 해결책들:
1. **EMR with Spark**: Athena 데이터 카탈로그 활용 가능
2. **Glue Spark Jobs**: Athena 테이블 직접 읽기/쓰기
3. **SageMaker with Spark**: 노트북에서 Spark 클러스터 생성

Athena Workgroup 한계:
- SQL 쿼리만 지원
- Spark 코드 실행 불가
- 쿼리 환경 격리 목적

시험 포인트:
- Athena = SQL 전용, Spark 실행 불가
- Spark 분석 = EMR/Glue/SageMaker 사용 필요

### 6. Glue Data Catalog 파티션 동기화 (최소 지연시간)

문제 시나리오: S3 파티션 경로 패턴, 새 파티션 추가 시 즉시 카탈로그 동기화

**해결책: Boto3 create_partition API 직접 호출**

```python
import boto3

glue = boto3.client('glue')

# 데이터 S3에 저장 후 즉시 파티션 등록
glue.create_partition(
    DatabaseName='my_database',
    TableName='my_table',
    PartitionInput={
        'Values': ['2023', '01', '01'],
        'StorageDescriptor': {
            'Location': 's3://bucket/prefix/year=2023/month=01/day=01/',
            'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',
            'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',
            'SerdeInfo': {'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'}
        }
    }
)
```

다른 방법들과 지연시간 비교:
- **Glue Crawler**: 스케줄 기반, 최소 5분 지연
- **자동 파티션 검색**: 쿼리 시점 검색, 첫 번째 쿼리 지연
- **직접 API 호출**: 즉시 등록, 지연시간 최소

시험 포인트:
- 최소 지연시간 요구사항 = 직접 API 호출
- 실시간 파티션 가용성 필요 시나리오

### 7. SaaS 데이터 통합 (최소 운영 오버헤드)

문제 시나리오: 서드파티 SaaS 도구 → S3 → Redshift 분석

**해결책: Amazon AppFlow**

AppFlow 특징:
- 코드 없는 데이터 통합: GUI 기반 설정
- 사전 구축된 커넥터: 40+ SaaS 애플리케이션 지원
- 자동 스케줄링: 정기적 데이터 동기화
- 데이터 변환: 기본적인 매핑 및 필터링

지원 SaaS 소스:
- Salesforce, ServiceNow, Slack
- Google Analytics, Adobe Analytics
- SAP, Zendesk, Marketo

대안 방법들:
- **Lambda + API**: 개발 오버헤드 높음
- **Glue ETL**: SaaS 연결 복잡
- **Third-party ETL**: 추가 비용 및 관리

시험 포인트:
- SaaS 통합 + 최소 운영 오버헤드 = AppFlow

### 8. S3 Parquet 단일 컬럼 조회 (최소 운영 오버헤드)

문제 시나리오: S3의 Parquet 파일에서 1개 컬럼만 조회, 일회성 작업

**해결책: S3 Select 사용**

S3 Select 특징:
- 서버리스 쿼리: 인프라 설정 불필요
- 컬럼 선택적 스캔: Parquet의 열 지향 구조 활용
- SQL 문법: 간단한 SELECT 문으로 조회
- 비용 효율적: 스캔된 데이터만 과금

```sql
-- S3 Select 쿼리 예시
SELECT column_name FROM s3object
WHERE condition
```

다른 방법들과 운영 오버헤드 비교:
- **Athena**: 데이터 카탈로그 설정 필요
- **Glue ETL**: 작업 정의 및 실행 환경 구성
- **EMR**: 클러스터 생성 및 관리
- **S3 Select**: 설정 없이 즉시 사용

시험 포인트:
- 일회성 작업 + 최소 운영 오버헤드 = S3 Select
- Parquet + 컬럼 선택적 조회에 최적

### 9. Redshift Materialized View 자동 새로고침 (최소 노력)

문제 시나리오: Redshift Materialized View 새로고침 일정 자동화

**해결책: Query Editor v2 사용**

Query Editor v2 기능:
- 스케줄링: GUI 기반 새로고침 스케줄 설정
- 내장 도구: 별도 서비스 구성 불필요
- 자동화: Cron 표현식으로 정기 실행
- 모니터링: 실행 상태 및 히스토리 추적

설정 단계:
1. Query Editor v2에서 Materialized View 선택
2. Schedule Refresh 옵션 활성화
3. 새로고침 빈도 설정 (시간별, 일별 등)

대안 방법들:
- **Lambda + Scheduler**: 개발 및 배포 필요
- **Step Functions**: 워크플로우 정의 및 관리
- **Airflow**: 별도 오케스트레이션 도구 설치

시험 포인트:
- Redshift MV 자동화 + 최소 노력 = Query Editor v2

### 10. Hadoop → EMR 마이그레이션 + 데이터 카탈로그 (서버리스)

문제 시나리오: 온프레미스 Hadoop + Hive 메타스토어 → EMR + 서버리스 카탈로그

**해결책: EMR Hive 메타스토어 + Glue Data Catalog 외부 카탈로그**

마이그레이션 아키텍처:
1. **온프레미스 → EMR 임시 메타스토어**
   - EMR 클러스터에 Hive 메타스토어 구성
   - 기존 메타데이터를 MySQL/RDS로 마이그레이션
   
2. **EMR → Glue Data Catalog**
   - EMR의 Hive 메타스토어를 Glue로 Export
   - External Hive Metastore로 Glue 설정
   
3. **서버리스 전환**
   - EMR 클러스터 종료
   - Glue Data Catalog만 유지

비용 효율성:
- **EMR 메타스토어**: 임시 마이그레이션용, 단기 비용
- **Glue Data Catalog**: 서버리스, 사용량 기반 과금
- **RDS 메타스토어**: 지속적인 인스턴스 비용 발생

Glue External Catalog 설정:
```json
{
  "hive-site": {
    "javax.jdo.option.ConnectionURL": "jdbc:mysql://glue-catalog-endpoint",
    "hive.metastore.client.factory.class": "com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory"
  }
}
```

시험 포인트:
- 서버리스 요구사항 + 비용 효율성 = Glue Data Catalog
- 단계적 마이그레이션: EMR 임시 → Glue 영구

### 11. 동적 JSON 스키마 + 데이터 카탈로그 (비용 최적화)

문제 시나리오: IoT JSON 데이터, 디바이스 업그레이드로 스키마 변경, 분석용 데이터 인덱싱

**해결책: Glue Data Catalog + Schema Registry + Redshift Serverless**

아키텍처 구성:
1. **Glue Data Catalog**: 메타데이터 중앙 저장소
2. **Glue Schema Registry**: JSON 스키마 버전 관리
3. **Glue ETL**: S3 → Redshift Serverless 데이터 로드
4. **Redshift Serverless**: 분석용 데이터웨어하우스

Schema Registry 장점:
- **스키마 진화**: 새로운 필드 추가/변경 자동 처리
- **버전 관리**: 이전 스키마와 호환성 유지
- **자동 검증**: 데이터 품질 보장
- **비용 효율적**: 스키마 변경 시 수동 작업 불필요

```json
// Schema Registry 스키마 예시
{
  "type": "record",
  "name": "IoTData",
  "fields": [
    {"name": "deviceId", "type": "string"},
    {"name": "timestamp", "type": "long"},
    {"name": "temperature", "type": ["null", "double"], "default": null},
    {"name": "humidity", "type": ["null", "double"], "default": null}
  ]
}
```

비용 최적화 요소:
- **Glue Data Catalog**: 사용량 기반 과금
- **Schema Registry**: 스키마별 과금, 저비용
- **Redshift Serverless**: 사용한 컴퓨팅만 과금
- **자동화**: 수동 스키마 관리 비용 제거

데이터 흐름:
1. IoT 데이터 → S3 (JSON)
2. Glue Crawler → 스키마 자동 감지
3. Schema Registry → 스키마 버전 등록
4. Glue ETL → 데이터 변환 및 로드
5. Redshift Serverless → 분석 쿼리

시험 포인트:
- 동적 스키마 + 분석 + 비용 최적화 = Glue + Schema Registry + Redshift Serverless

### 12. S3 쓰기 작업 로깅 (최소 운영 노력)

문제 시나리오: S3 버킷 쓰기 작업을 다른 S3 버킷에 로그로 기록

**해결책: CloudTrail 데이터 이벤트 추적**

CloudTrail 데이터 이벤트 구성:
- **Trail 생성**: S3 데이터 이벤트 전용 추적
- **빈 prefix**: 모든 객체에 대한 이벤트 캡처
- **Write-only 이벤트**: PUT, POST, DELETE 작업만 기록
- **대상 버킷**: 로그 파일 저장 위치

CloudTrail 설정 예시:
```json
{
  "Trail": {
    "Name": "S3WriteTrail",
    "S3BucketName": "logs-bucket",
    "IncludeGlobalServiceEvents": false,
    "IsMultiRegionTrail": false,
    "EventSelectors": [{
      "ReadWriteType": "WriteOnly",
      "IncludeManagementEvents": false,
      "DataResources": [{
        "Type": "AWS::S3::Object",
        "Values": ["arn:aws:s3:::transactions-bucket/*"]
      }]
    }]
  }
}
```

다른 방법들과 운영 노력 비교:
- **S3 이벤트 알림 + Lambda**: 개발 및 유지보수 필요
- **VPC Flow Logs**: 네트워크 레벨, S3 작업 세부사항 부족
- **CloudWatch Logs**: 별도 로깅 에이전트 구성 필요
- **CloudTrail**: GUI 설정, 자동 로그 수집

시험 포인트:
- S3 작업 감사 + 최소 운영 노력 = CloudTrail 데이터 이벤트

### 13. 데이터 레이크 세분화된 액세스 제어 (최소 운영 오버헤드)

문제 시나리오: 팀별 행/열 수준 데이터 액세스 제어, Athena/Redshift Spectrum/EMR Hive 지원

**해결책: S3 + Lake Formation 통합 액세스 제어**

Lake Formation 세분화된 제어:
- **행 수준 보안**: WHERE절 자동 적용으로 팀별 데이터 필터링
- **열 수준 보안**: 민감한 컬럼 마스킹 또는 숨김
- **통합 액세스**: 모든 분석 도구에서 동일한 권한 적용
- **중앙집중식 관리**: 단일 콘솔에서 모든 권한 설정

아키텍처:
```
S3 Data Lake
    ↓
Lake Formation (권한 엔진)
    ↓
┌─────────────┬─────────────────┬─────────────┐
│   Athena    │ Redshift Spectrum│ EMR Hive    │
│(자동 필터링)│    (자동 필터링)   │(자동 필터링) │
└─────────────┴─────────────────┴─────────────┘
```

권한 설정 예시:
- **팀 A**: customer 테이블의 region='us-east-1' 행만, PII 컬럼 제외
- **팀 B**: 전체 데이터 읽기, email 컬럼 마스킹
- **팀 C**: 집계 데이터만, 개별 레코드 접근 불가

대안 방법들의 운영 오버헤드:
- **IAM + S3 버킷 정책**: 세분화된 제어 제한적
- **각 도구별 개별 권한**: Athena, Redshift, EMR 각각 설정 필요
- **프록시 계층**: 추가 인프라 및 개발 필요

시험 포인트:
- 다중 분석 도구 + 세분화된 권한 + 최소 운영 오버헤드 = Lake Formation

### 14. Redshift 디스크 공간 회수 + Sort Key 분석

문제 시나리오: 6개월간 UPDATE/DELETE 작업, interleaved sort key, 디스크 공간 회수 + sort key 분석

**해결책: VACUUM REINDEX 명령**

VACUUM REINDEX 기능:
- **공간 회수**: DELETE된 행과 UPDATE로 생긴 빈 공간 재확보
- **Sort Key 재정렬**: interleaved sort key 순서 최적화
- **통계 갱신**: sort key 분석을 위한 테이블 통계 새로고침
- **성능 개선**: 쿼리 성능 향상

```sql
-- VACUUM REINDEX 실행
VACUUM REINDEX Orders;

-- Sort key 효율성 확인
SELECT "table", unsorted, vacuum_sort_benefit 
FROM SVV_TABLE_INFO 
WHERE "table" = 'orders';
```

Redshift VACUUM 옵션 비교:
- **VACUUM**: 기본 공간 회수, sort key 재정렬 안함
- **VACUUM SORT ONLY**: Sort key만 재정렬, 공간 회수 안함
- **VACUUM DELETE ONLY**: 공간 회수만, sort key 재정렬 안함
- **VACUUM REINDEX**: 공간 회수 + sort key 재정렬 + 통계 갱신

Interleaved Sort Key 특징:
- **다차원 정렬**: 여러 컬럼 조합 쿼리에 효과적
- **유지보수 필요**: 정기적 REINDEX로 성능 유지
- **분석 가능**: 통계 테이블로 효율성 측정

시험 포인트:
- 디스크 공간 회수 + sort key 분석 = VACUUM REINDEX


### 15. 다중 소스 ETL 파이프라인 + 스키마 변경 대응

문제 시나리오: 10개 소스 시스템, 15분마다 CSV/JSON/Parquet 파일 생성, 10MB-20GB 크기, 스키마 변경 대응

**해결책: EventBridge/Lambda + Glue Workflow + Crawler 자동 트리거**

**방법 1: 시간 기반 처리 (EventBridge)**
```json
{
  "EventBridge Rule": "rate(15 minutes)",
  "Target": "Glue Workflow",
  "Workflow": {
    "Trigger1": "Schedule Trigger",
    "Crawler": "스키마 자동 감지",
    "Trigger2": "On-demand (Crawler 성공 시)",
    "Job": "데이터 변환 및 Redshift 로드"
  }
}
```

**방법 2: 이벤트 기반 처리 (Lambda)**
```python
def lambda_handler(event, context):
    # S3 PUT 이벤트 수신
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    # Glue Workflow 실행
    glue.start_workflow_run(Name='ETL-Workflow')
```

Glue Workflow 구성:
1. **Glue Crawler**: 새 파일의 스키마 자동 감지 및 카탈로그 업데이트
2. **On-demand Trigger**: Crawler 성공 시 ETL Job 자동 실행
3. **Glue ETL Job**: 
   - 동적 스키마 적용
   - 다중 파일 형식 처리
   - Redshift COPY 또는 JDBC 연결

스키마 변경 대응 메커니즘:
- **Crawler 자동 감지**: 새로운 컬럼, 데이터 타입 변경 자동 탐지
- **Dynamic Frame**: 스키마 변경에 유연한 데이터 구조
- **Schema Evolution**: 기존 테이블과 신규 스키마 호환성 관리

대용량 파일 처리 최적화:
- **파티셔닝**: 날짜/소스별 파티션으로 병렬 처리
- **DPU 자동 스케일링**: 파일 크기에 따른 리소스 조정
- **Redshift COPY**: 병렬 로드로 성능 최적화

시험 포인트:
- 다중 소스 + 스키마 변경 + 자동화 = Glue Workflow + Crawler



A company's data engineer needs to optimize the performance of table SQL queries. The company stores data in an Amazon Redshift cluster. The data engineer cannot increase the size of the cluster because of budget constraints.
The company stores the data in multiple tables and loads the data by using the EVEN distribution style. Some tables are hundreds of gigabytes in size. Other tables are less than 10 MB in size.
Which solution will meet these requirements?

### 16. Redshift 쿼리 성능 최적화 (예산 제약)

문제 시나리오: 클러스터 크기 증가 불가, EVEN 분산 스타일, 대형 테이블(수백GB) + 소형 테이블(10MB 미만)

**해결책: 소형 테이블 ALL 분산 + Primary/Foreign Key 설정**

**ALL 분산 스타일 장점:**
- **로컬 조인**: 모든 노드에 복사되어 네트워크 I/O 제거
- **소형 테이블 최적화**: 10MB 미만 차원 테이블에 이상적
- **조인 성능 향상**: 대형 팩트 테이블과 조인 시 재분산 불필요

```sql
-- 소형 차원 테이블: ALL 분산
ALTER TABLE small_dimension_table 
ALTER DISTSTYLE ALL;

-- Primary/Foreign Key 설정
ALTER TABLE customers ADD PRIMARY KEY (customer_id);
ALTER TABLE orders ADD FOREIGN KEY (customer_id) 
  REFERENCES customers(customer_id);
```

분산 스타일 비교:
- **EVEN**: 라운드로빈 분산, 균등하지만 조인 시 재분산
- **KEY**: 특정 컬럼 기준 분산, 조인 최적화
- **ALL**: 전체 복사, 소형 테이블 전용

시험 포인트:
- 예산 제약 + 소형/대형 테이블 혼재 = ALL 분산 + 키 제약 조건

### 17. AWS Glue DataBrew NEST_TO_MAP 변환

**NEST_TO_MAP 변환 설명:**

정의: 중첩된 JSON 객체나 배열을 키-값 쌍의 맵으로 변환하는 DataBrew 변환 함수

변환 예시:
```json
// 원본 데이터
{
  "user": {
    "profile": {
      "name": "John",
      "preferences": {
        "color": "blue",
        "food": "pizza"
      }
    }
  }
}

// NEST_TO_MAP 변환 후
{
  "user_profile_map": {
    "name": "John",
    "preferences.color": "blue", 
    "preferences.food": "pizza"
  }
}
```

사용 사례:
- **IoT 센서 데이터**: 다양한 센서 값을 맵으로 정규화
- **사용자 프로필**: 중첩된 설정을 분석 가능한 형태로 변환
- **로그 데이터**: 가변적인 메타데이터를 구조화된 맵으로 변환

시험 포인트:
- 중첩 JSON → 분석 가능한 맵 구조 = NEST_TO_MAP 변환

A company receives call logs as Amazon S3 objects that contain sensitive customer information. The company must protect the S3 objects by using encryption. The company must also use encryption keys that only specific employees can access.
Which solution will meet these requirements with the LEAST effort?

### 18. S3 민감 데이터 암호화 + 직원별 액세스 제어

문제 시나리오: 고객 정보 포함 통화 로그, 특정 직원만 암호화 키 접근

**해결책: SSE-KMS (정답) vs SSE-S3 (오답) 비교**

**왜 SSE-KMS가 정답인가:**
- **고객 관리 키**: 암호화 키에 대한 완전한 제어
- **세분화된 권한**: 키별 IAM 정책 적용 가능
- **감사 추적**: CloudTrail로 키 사용 로그 기록
- **키 교체**: 자동/수동 키 교체 지원

**왜 SSE-S3는 불가능한가:**
- **AWS 완전 관리**: S3 관리 키는 AWS가 완전 제어
- **IAM 제어 불가**: S3 관리 키에 대한 IAM 정책 적용 불가능
- **키 접근 제한 불가**: 특정 직원만 접근하도록 제한할 방법 없음
- **투명한 암호화**: 키 관리가 사용자에게 노출되지 않음

```json
// KMS 키 정책 예시 (가능)
{
  "Version": "2012-10-17",
  "Statement": [{
    "Sid": "AllowSpecificEmployees",
    "Effect": "Allow",
    "Principal": {
      "AWS": [
        "arn:aws:iam::account:user/employee1",
        "arn:aws:iam::account:user/employee2"
      ]
    },
    "Action": ["kms:Encrypt", "kms:Decrypt"],
    "Resource": "*"
  }]
}

// S3 관리 키 정책 (불가능)
// S3 관리 키는 사용자가 정책을 설정할 수 없음
```

**암호화 옵션 비교:**
- **SSE-S3**: AWS 관리, 키 제어 불가, 최소 노력
- **SSE-KMS**: 고객 관리, 세분화된 제어, 약간의 설정 필요
- **SSE-C**: 고객 제공, 완전한 제어, 높은 관리 부담

시험 포인트:
- 특정 직원 액세스 제한 = SSE-KMS 필수
- SSE-S3 = 키 접근 제어 불가능

### 19. AWS Glue DataBrew - 시각적 데이터 준비 서비스

문제 시나리오: 일일 2GB XLS 파일, 이름 컬럼 결합, 고유 고객 수 계산

**AWS Glue DataBrew 개요:**

정의: 코드 작성 없이 시각적 인터페이스로 데이터를 정리하고 변환하는 서버리스 데이터 준비 서비스

핵심 기능:
- **비주얼 인터페이스**: 드래그 앤 드롭으로 데이터 변환
- **자동 스키마 감지**: Excel, CSV, JSON 등 다양한 형식 지원
- **데이터 프로파일링**: 품질 문제 자동 탐지
- **Recipe 기반**: 재사용 가능한 변환 단계 정의

**DataBrew Recipe 구성:**
```json
{
  "Recipe": [
    {
      "Action": "CONCATENATE",
      "Parameters": {
        "sourceColumns": ["first_name", "last_name"],
        "targetColumn": "full_name",
        "delimiter": " "
      }
    },
    {
      "Action": "COUNT_DISTINCT",
      "Parameters": {
        "sourceColumn": "full_name",
        "targetColumn": "distinct_customers"
      }
    }
  ]
}
```

**다른 방법들과 운영 노력 비교:**
- **AWS Glue ETL**: Spark/Python 코드 작성 필요
- **Lambda**: 코드 개발, 메모리 제한 (최대 10GB)
- **EMR**: 클러스터 설정 및 관리
- **DataBrew**: GUI 기반, 코드 불필요

**DataBrew 장점:**
- **Excel 네이티브 지원**: .xls/.xlsx 파일 직접 읽기
- **대용량 처리**: 서버리스 확장으로 2GB 파일 처리
- **내장 함수**: COUNT_DISTINCT 같은 집계 함수 제공
- **프로파일링**: 데이터 품질 및 분포 자동 분석

사용 시나리오:
- 비개발자 데이터 분석가용
- 일회성 데이터 탐색
- 반복적인 데이터 정리 작업

시험 포인트:
- Excel + 데이터 변환 + 최소 운영 노력 = Glue DataBrew

### 20. Amazon Redshift Streaming Ingestion - 실시간 스트림 수집

문제 시나리오: Kinesis Data Streams 헬스 데이터, Redshift Serverless 저장, 실시간 + 과거 데이터 분석

**Redshift Streaming Ingestion 개요:**

정의: Kinesis Data Streams에서 직접 Redshift로 실시간 데이터를 수집하는 기능

핵심 특징:
- **직접 통합**: 중간 ETL 단계 없이 Kinesis → Redshift 직접 연결
- **실시간 처리**: 초 단위 지연으로 스트리밍 데이터 수집
- **자동 스키마 매핑**: JSON 데이터를 테이블 스키마로 자동 변환
- **서버리스 호환**: Redshift Serverless와 완전 통합

**Streaming Ingestion 설정:**

```sql
-- Streaming 테이블 생성
CREATE MATERIALIZED VIEW health_data_stream AS
SELECT 
    kinesis_shardid,
    kinesis_sequence_number,
    kinesis_timestamp,
    JSON_EXTRACT_PATH_TEXT(kinesis_data, 'device_id') as device_id,
    JSON_EXTRACT_PATH_TEXT(kinesis_data, 'heart_rate') as heart_rate,
    JSON_EXTRACT_PATH_TEXT(kinesis_data, 'timestamp') as event_time
FROM (
    SELECT * FROM STREAM('arn:aws:kinesis:region:account:stream/health-data-stream')
);

-- 실시간 + 과거 데이터 분석 쿼리
SELECT device_id, AVG(heart_rate::int) as avg_heart_rate
FROM health_data_stream
WHERE event_time >= DATEADD(hour, -24, GETDATE())
GROUP BY device_id;
```

**아키텍처:**
```
웨어러블 기기 → Kinesis Data Streams → Redshift Streaming Ingestion → Materialized View
        ↓
병원 장비 → Auto Refresh → 실시간 분석 쿼리
        ↓
환자 기록 → 과거 데이터와 조인
```

**다른 방법들과 운영 오버헤드 비교:**
- **Kinesis Data Firehose**: 배치 처리, 실시간성 제한
- **Lambda + COPY**: 개발 필요, 관리 복잡성
- **Glue Streaming**: 별도 ETL 작업 구성 필요
- **Streaming Ingestion**: 설정만으로 완료

**실시간 + 과거 데이터 통합:**
- **Materialized View**: 스트림 데이터 자동 집계
- **Auto Refresh**: 새 데이터 도착 시 즉시 갱신
- **Historical Join**: 기존 테이블과 실시간 조인 가능
- **Time Travel**: 과거 특정 시점 데이터 분석

성능 최적화:
- **파티셔닝**: 시간 기반 자동 파티션
- **압축**: 자동 데이터 압축
- **병렬 처리**: 다중 샤드 동시 처리

시험 포인트:
- Kinesis → Redshift + 실시간 분석 + 최소 운영 오버헤드 = Streaming Ingestion

### 21. 다중 데이터 소스 통합 쿼리 - Athena + PartiQL

문제 시나리오: S3 (JSON/CSV) + RDS SQL Server + DynamoDB + Redshift, 통합 SQL 쿼리 인터페이스

**해결책: Glue Data Catalog + Athena + PartiQL 통합**

**아키텍처 구성:**

```
┌─ S3 (JSON/CSV) ─┐    ┌─ AWS Glue Crawlers ─┐    ┌─ Glue Data Catalog ─┐
├─ RDS SQL Server ├ →  ├─ 메타데이터 수집    ├ →  ├─ 통합 메타데이터   ├ → Athena
├─ DynamoDB      ─┤    ├─ 스키마 자동 감지   ─┤    ├─ 테이블 정의      ─┤   ↓
└─ Redshift      ─┘    └─ 연결정보 관리     ─┘    └─ 연결정보 저장    ─┘  Query
```

**각 데이터 소스별 통합 방법:**

**1. S3 데이터 (JSON/CSV)**
```sql
-- Glue Crawler가 자동 생성한 테이블
-- CSV 데이터
SELECT customer_id, order_amount 
FROM s3_csv_data 
WHERE order_date > '2023-01-01';

-- JSON 데이터 (PartiQL 사용)
SELECT customer_id, 
       json_extract_scalar(metadata, '$.purchase_category') as category
FROM s3_json_data 
WHERE json_extract_scalar(metadata, '$.region') = 'us-east-1';
```

**2. RDS SQL Server 연결**
```sql
-- External Table로 RDS 연결
CREATE EXTERNAL TABLE rds_customers (
    customer_id INT,
    customer_name STRING,
    registration_date DATE
)
STORED BY 'org.apache.hadoop.hive.jdbc.storagehandler.JdbcStorageHandler'
TBLPROPERTIES (
    "hive.sql.database.type" = "MSSQL",
    "hive.sql.jdbc.driver" = "com.microsoft.sqlserver.jdbc.SQLServerDriver",
    "hive.sql.jdbc.url" = "jdbc:sqlserver://rds-endpoint:1433;database=prod",
    "hive.sql.dbcp.username" = "username",
    "hive.sql.dbcp.password" = "password",
    "hive.sql.table" = "customers"
);
```

**3. DynamoDB 연결**
```sql
-- DynamoDB Connector를 통한 연결
CREATE EXTERNAL TABLE dynamo_user_profiles (
    user_id STRING,
    profile_data STRING,  -- JSON 형태
    last_activity DATE
)
STORED BY 'org.apache.hadoop.hive.dynamodb.DynamoDBStorageHandler'
TBLPROPERTIES (
    "dynamodb.table.name" = "user-profiles",
    "dynamodb.column.mapping" = "user_id:userId,profile_data:profileData,last_activity:lastActivity"
);
```

**4. Redshift 연결**
```sql
-- Redshift External Schema 사용
CREATE EXTERNAL SCHEMA redshift_schema
FROM REDSHIFT 
DATABASE 'analytics_db' 
URI 'redshift-cluster-endpoint:5439'
IAM_ROLE 'arn:aws:iam::account:role/RedshiftRole';

-- Redshift 테이블 쿼리
SELECT product_id, SUM(sales_amount) as total_sales
FROM redshift_schema.sales_fact
GROUP BY product_id;
```

**통합 쿼리 예시:**
```sql
-- 모든 데이터 소스를 조인한 통합 쿼리
SELECT 
    c.customer_name,
    s.order_amount,
    JSON_EXTRACT_SCALAR(p.profile_data, '$.preferences') as preferences,
    r.total_sales
FROM rds_customers c
JOIN s3_csv_data s ON c.customer_id = s.customer_id
JOIN dynamo_user_profiles p ON c.customer_id = p.user_id
JOIN redshift_schema.customer_sales r ON c.customer_id = r.customer_id
WHERE c.registration_date > '2023-01-01';
```

**PartiQL의 역할:**
- **JSON 쿼리**: JSON 필드에 SQL-like 문법 적용
- **중첩 데이터**: 복잡한 JSON 구조 탐색
- **배열 처리**: JSON 배열 요소 개별 접근
- **타입 변환**: JSON 값의 자동 타입 캐스팅

**Glue Data Catalog의 역할:**
- **메타데이터 통합**: 모든 데이터 소스의 스키마 정보 중앙화
- **자동 스키마 감지**: Crawler를 통한 스키마 자동 업데이트  
- **연결 정보 관리**: 다양한 데이터 소스 연결 설정 저장
- **권한 관리**: Lake Formation과 통합된 액세스 제어

**운영 오버헤드 최소화:**
- **Crawler 자동화**: 스케줄 기반 메타데이터 자동 갱신
- **서버리스**: 인프라 관리 불필요
- **표준 SQL**: 기존 SQL 지식 활용 가능
- **통합 인터페이스**: 단일 쿼리 엔진으로 모든 소스 접근

시험 포인트:
- 다중 데이터 소스 + SQL 통합 쿼리 + 최소 운영 오버헤드 = Athena + Glue + PartiQL
